{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.layers import Layer, InputSpec, ReLU\n",
    "from tensorflow.keras.initializers import Ones\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAct(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MAct, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.c = self.add_weight(name=\"c\",\n",
    "                                shape=(input_shape[1],),\n",
    "                                initializer='ones',\n",
    "                                trainable=True) # Initialiseerida c Ã¼htedeks / nullideks\n",
    "        self.b = self.add_weight(name=\"b\",\n",
    "                                shape=(input_shape[1],),\n",
    "                                initializer='ones',\n",
    "                                trainable=True) # Initialiseerida b nullideks\n",
    "        super(MAct, self).build(input_shape)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        first_exp = tf.exp(self.c - tf.square(inputs))\n",
    "\n",
    "        p = (first_exp + tf.exp(self.b)) / tf.reduce_sum(first_exp + tf.exp(self.b), axis=1, keepdims=True)\n",
    "        \n",
    "        #p = tf.exp(inputs) / tf.reduce_sum(tf.exp(inputs), axis=0, keepdims=True)\n",
    "        return p\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'c_initializer': initializers.serialize(self.c_initializer),\n",
    "            'b_initializer': initializers.serialize(self.b_initializer),\n",
    "        }\n",
    "        base_config = super(MAct, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(probs, y):\n",
    "    #losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y) # Tavaline CE\n",
    "    cce = CategoricalCrossentropy()\n",
    "    losses = cce(probs, y)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "\n",
    "def max_conf(probs, dim):\n",
    "    y = tf.argmax(probs, 1)\n",
    "    y = tf.one_hot(y, dim)\n",
    "    #losses = -tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y) # Tavaline CE\n",
    "    cce = CategoricalCrossentropy()\n",
    "    losses = -cce(probs, y)\n",
    "    return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(x, dim):\n",
    "    eps = 0.025\n",
    "    n_iters = 4\n",
    "    step_size = 0.02\n",
    "\n",
    "    unif = tf.random.uniform(minval=-eps, maxval=eps, shape=tf.shape(x))\n",
    "    x_adv = tf.clip_by_value(x + unif, 0., 1.)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        x_adv = tf.Variable(x_adv)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = max_conf(model(x_adv), dim)\n",
    "            grad = tape.gradient(loss, x_adv)\n",
    "            g = tf.sign(grad)\n",
    "\n",
    "        x_adv_start = x_adv + step_size*g\n",
    "        x_adv = tf.clip_by_value(x_adv, 0., 1.)\n",
    "        delta = x_adv - x_adv_start\n",
    "        delta = tf.clip_by_value(delta, -eps, eps)\n",
    "        x_adv = x_adv_start + delta\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, plot_min, plot_max, max_prob, layers, X, y):\n",
    "    n_grid = 200\n",
    "    x_plot = np.linspace(plot_min, plot_max, n_grid)\n",
    "    y_plot = np.linspace(plot_min, plot_max, n_grid)\n",
    "    \n",
    "    points = []\n",
    "    for xx in x_plot:\n",
    "        for yy in y_plot:\n",
    "            points.append((yy, xx))\n",
    "    points = np.array(points)\n",
    "    \n",
    "    probs = model(points).numpy()\n",
    "    #probs = tf.nn.softmax(logits).numpy()\n",
    "    if max_prob:\n",
    "        z_plot = probs.max(1)\n",
    "    else:\n",
    "        z_plot = probs[:, 0]\n",
    "    z_plot = z_plot.reshape(len(x_plot), len(y_plot)) * 100\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    vmax = 100\n",
    "    vmin = 50 if max_prob else 0\n",
    "    plt.contourf(x_plot, y_plot, z_plot, levels=np.linspace(50, 100, 50))\n",
    "    cbar = plt.colorbar(ticks=np.linspace(vmin, vmax, 6))\n",
    "    \n",
    "    cbar.ax.set_title('confidence', fontsize=12, pad=12)\n",
    "    cbar.set_ticklabels(['50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "    \n",
    "    y_np = np.array(y)\n",
    "    X0 = X[y_np.argmax(1)==0]\n",
    "    X1 = X[y_np.argmax(1)==1]\n",
    "    plt.scatter(X0[:, 0], X0[:, 1], s=20, edgecolors='red', facecolor='None',\n",
    "                marker='o', linewidths=0.2)\n",
    "    plt.scatter(X1[:, 0], X1[:, 1], s=20, edgecolors='green', facecolor='None',\n",
    "                marker='s', linewidths=0.2)\n",
    "    plt.xlim([plot_min, plot_max])\n",
    "    plt.ylim([plot_min, plot_max])\n",
    "    \n",
    "    margin = 0.01\n",
    "    rect = matplotlib.patches.Rectangle((-margin, -margin), 1.0+2*margin, 1.0+2*margin, \n",
    "                                        linewidth=1.5, color='white', fill=False)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.savefig('{}_{:.1f}_{:.1f}_max_prob={}.pdf'.format(\n",
    "        layers, plot_min, plot_max, max_prob), transparent=True)\n",
    "    plt.clf()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experiment(model, layers, optimizer, acet):\n",
    "    for n_iter in [200, 400, 600, 800, 1000]:\n",
    "        dim = 2\n",
    "        # More noise in the moons makes the task harder\n",
    "        X, y = datasets.make_moons(n_samples=2000, shuffle=True, noise=.02)\n",
    "        # Rescale and shift the dataset to better fit into zero-one box\n",
    "        X = (X + 1.6) / 4\n",
    "        X[:, 0] = X[:, 0] - 0.035\n",
    "        X[:, 1] = (X[:, 1] - 0.17) * 1.75\n",
    "        y = tf.one_hot(y, dim)\n",
    "\n",
    "        X_test, y_test = datasets.make_moons(n_samples=400, shuffle=True, noise=.02)\n",
    "        X_test = (X_test + 1.6) / 4\n",
    "        X_test[:, 0] = X_test[:, 0] - 0.035\n",
    "        X_test[:, 1] = (X_test[:, 1] - 0.17) * 1.75\n",
    "\n",
    "        info_list = []\n",
    "\n",
    "        # Custom training cycle going through the entire dataset\n",
    "        for epoch in range(1, n_iter+1):\n",
    "            X_noise = tf.random.uniform([2*X.shape[0], X.shape[1]])\n",
    "            # If we use the ACET method, then adversarial noise will be generated\n",
    "            if acet:\n",
    "                X_noise = gen_adv(X_noise, dim)\n",
    "            # Context used to calculate the gradients of the model\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(X)\n",
    "                logits_noise = model(X_noise)\n",
    "                loss_main = cross_ent(logits, y)\n",
    "                loss_acet = acet * max_conf(logits_noise, dim)\n",
    "                loss = loss_main + loss_acet\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            if epoch % 100  == 0:\n",
    "                train_err = np.mean(logits.numpy().argmax(1) != y.numpy().argmax(1))\n",
    "                #print(\"Iter {:03d}: loss_main={:.10f} loss_acet={:.3f} err={:.2%}\"\n",
    "                #      .format(epoch, loss_main, loss_acet, train_err))\n",
    "\n",
    "                weights = model.layers[-1].get_weights()\n",
    "                info_list.append(\"Iter {:03d}: loss_main={:.10f} loss_acet={:.6f} err={:.2%} c: {}, b: {}\"\n",
    "                                 .format(epoch, loss_main, loss_acet, train_err, weights[0], weights[1]))\n",
    "            \n",
    "        #logits = model(X_test)\n",
    "        #loss_test = cross_ent(logits, y_test)\n",
    "        #info_list.append(\"Test: loss_test={:.10f}\".format(loss_test))\n",
    "\n",
    "        file_name = \"one_train_one_train/{}_iters={}.csv\".format(layers, n_iter)\n",
    "        with open(file_name, 'w', newline='') as myfile:\n",
    "            wr = csv.writer(myfile, delimiter=\"\\n\")\n",
    "            wr.writerow(info_list)\n",
    "\n",
    "        plot(model, 0.0, 1.0, True, layers + \"_iters={}\".format(n_iter), X, y)\n",
    "        #plot(model, 0.3, 0.5, max_prob=True, layers)\n",
    "        plot(model, -2.0, 3.0, True, layers + \"_iters={}\".format(n_iter), X, y)\n",
    "        plot(model, -5.0, 6.0, True, layers + \"_iters={}\".format(n_iter), X, y)\n",
    "        plot(model, -10.0, 10.0, True, layers + \"_iters={}\".format(n_iter), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_126 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_129 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_135 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_138 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_141 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_147 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_153 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_159 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_162 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_165 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_168 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_171 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_174 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_177 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_183 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_186 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_189 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_195 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_198 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_201 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_204 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_207 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_210 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_213 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_216 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_219 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_222 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_225 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_228 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_231 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_234 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_237 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_240 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_243 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_246 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_249 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_252 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_255 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_258 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_261 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_264 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_267 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_270 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_273 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_276 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_279 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_282 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_285 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_288 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_291 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_294 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_297 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_300 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_303 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_306 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_309 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_312 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_315 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_318 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_321 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_324 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_327 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_330 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_333 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_336 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_339 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_342 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_345 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_348 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_351 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_354 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_357 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_360 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_363 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_366 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_369 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_372 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_375 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_378 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_381 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_384 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_387 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_390 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_393 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_396 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many layers\n",
    "for number_layers in range(2,3):\n",
    "    # How many nodes in first dense\n",
    "    for dense_1 in range(20, 110, 20):\n",
    "        # What is the first activation layer\n",
    "        for activation_1 in ['selu', 'sigmoid', 'relu', 'tanh']:\n",
    "            # If the number of hidden layers is 1\n",
    "            if number_layers == 1:\n",
    "                for l_rate in [0.001, 0.01, 0.1]:\n",
    "                    for acet in [True, False]:\n",
    "                        model = Sequential([\n",
    "                            Dense(dense_1, input_shape=(2,)),\n",
    "                            Activation(activation_1),\n",
    "                            Dense(2),\n",
    "                            MAct()\n",
    "                        ])\n",
    "                        optim = Adam(learning_rate=l_rate)\n",
    "                        lay = \"d={}_act={}_l_rate={}_acet={}\".format(dense_1, activation_1, l_rate, acet)\n",
    "                        model_experiment(model, lay, optim, acet)\n",
    "            # If the number of hidden layers is 2\n",
    "            else:\n",
    "                for dense_2 in range(100, 110, 20):\n",
    "                    for activation_2 in ['selu', 'sigmoid', 'relu', 'tanh']:\n",
    "                        for l_rate in [0.001, 0.01, 0.1]:\n",
    "                            for acet in [True, False]:\n",
    "                                model = Sequential([\n",
    "                                    Dense(dense_1, input_shape=(2,)),\n",
    "                                    Activation(activation_1),\n",
    "                                    Dense(dense_2),\n",
    "                                    Activation(activation_2),\n",
    "                                    Dense(2),\n",
    "                                    MAct()\n",
    "                                ])\n",
    "                                optim = Adam(learning_rate=l_rate)\n",
    "                                lay = \"d={}_act={}_d={}_act={}_l_rate={}_acet={}\".format(dense_1, activation_1, \n",
    "                                                                                         dense_2, activation_2, l_rate, acet)\n",
    "                                model_experiment(model, lay, optim, acet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MAct.call of <__main__.MAct object at 0x7fdb5030c390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MAct.call of <__main__.MAct object at 0x7fdb5030c390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(100,input_shape=(2,)),\n",
    "    Activation('relu'),\n",
    "    Dense(100),\n",
    "    Activation('relu'),\n",
    "    Dense(2),\n",
    "    MAct(),\n",
    "    #Activation('softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "# More noise in the moons makes the task harder\n",
    "X, y = datasets.make_moons(n_samples=2000, shuffle=True, noise=.02)\n",
    "# Rescale and shift the dataset to better fit into zero-one box\n",
    "X = (X + 1.6) / 4\n",
    "X[:, 0] = X[:, 0] - 0.035\n",
    "X[:, 1] = (X[:, 1] - 0.17) * 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = datasets.make_moons(n_samples=400, shuffle=True, noise=.02)\n",
    "X_test = (X_test + 1.6) / 4\n",
    "X_test[:, 0] = X_test[:, 0] - 0.035\n",
    "X_test[:, 1] = (X_test[:, 1] - 0.17) * 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.one_hot(y, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]], shape=(2000, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "acet = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if acet:\n",
    "    n_iter = 900\n",
    "else:\n",
    "    n_iter = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Iter 100: loss_main=3.0446724892 loss_acet=-0.000 err=10.10%\n",
      "Iter 200: loss_main=0.5265916586 loss_acet=-0.000 err=0.00%\n"
     ]
    }
   ],
   "source": [
    "weights_list = []\n",
    "\n",
    "# Custom training cycle going through the entire dataset\n",
    "for epoch in range(1, n_iter+1):\n",
    "    X_noise = tf.random.uniform([2*X.shape[0], X.shape[1]])\n",
    "    # If we use the ACET method, then adversarial noise will be generated\n",
    "    if acet:\n",
    "        X_noise = gen_adv(X_noise)\n",
    "    # Context used to calculate the gradients of the model\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(X)\n",
    "        logits_noise = model(X_noise)\n",
    "        loss_main = cross_ent(logits, y)\n",
    "        loss_acet = acet * max_conf(logits_noise, dim)\n",
    "        loss = loss_main + loss_acet\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    if epoch % 100  == 0:\n",
    "        train_err = np.mean(logits.numpy().argmax(1) != y.numpy().argmax(1))\n",
    "        print(\"Iter {:03d}: loss_main={:.10f} loss_acet={:.3f} err={:.2%}\"\n",
    "              .format(epoch, loss_main, loss_acet, train_err))\n",
    "        \n",
    "        weights = model.layers[-1].get_weights()\n",
    "        weights_list.append(\"c: {}, b: {}\".format(weights[0], weights[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "\n",
    "for idx in range(len(X_test)):\n",
    "    sample_test = X_test[idx]\n",
    "    target_test = y_test[idx]\n",
    "\n",
    "    # Adding batch dim since batch=1\n",
    "    sample_test = np.expand_dims(sample_test, axis=0)\n",
    "    target_test = np.expand_dims(target_test, axis=0)\n",
    "\n",
    "    # To tensors\n",
    "    sample_test = K.constant(sample_test)\n",
    "    target_test = K.constant(target_test)\n",
    "        \n",
    "    logits = model(sample_test)\n",
    "    prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    #print(prediction.shape)\n",
    "    #print(target_test.shape)\n",
    "    test_accuracy(prediction, target_test)\n",
    "    \n",
    "\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c: [2.0707293 2.048267 ], b: [1. 1.]', 'c: [3.1125352 3.0621438], b: [1. 1.]', 'c: [3.871858  3.8434265], b: [1. 1.]', 'c: [4.376797 4.361787], b: [1. 1.]', 'c: [4.7614646 4.753905 ], b: [1. 1.]', 'c: [5.0730534 5.0698113], b: [1. 1.]', 'c: [5.3357415 5.3352914], b: [1. 1.]', 'c: [5.5635543 5.565012 ], b: [1. 1.]', 'c: [5.765257 5.768091], b: [1. 1.]']\n"
     ]
    }
   ],
   "source": [
    "print(weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mact_dense100_selu_dense100_selu_dense2_mact_no_acet_0.001_b_frozen_ones_z_abs900.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, delimiter=\"\\n\")\n",
    "    wr.writerow(weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(model, 0.0, 1.0, True,\"dense100_dense100_relu_MAct_iter=200\", X, y)\n",
    "#plot(model, 0.3, 0.5, max_prob=True, X, y)\n",
    "plot(model, -2.0, 3.0, True,\"dense100_dense100_relu_MAct_iter=200\", X, y)\n",
    "plot(model, -5.0, 6.0, True,\"dense100_dense100_relu_MAct_iter=200\", X, y)\n",
    "plot(model, -10.0, 10.0, True,\"dense100_dense100_relu_MAct_iter=200\", X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: [0.9785716 0.9210024]\n",
      "b: [-0.9543602 -0.9543602]\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[-1].get_weights()\n",
    "print('c:', weights[0])\n",
    "print('b:', weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.5428169 ,  0.23567562, -0.07776379,  0.04618938,  0.09044392,\n",
      "        -0.14229243, -0.20757382,  0.07028708, -0.15074916, -0.20390311,\n",
      "        -0.3845685 ,  0.39416412, -0.12063653,  0.05148071, -0.04625471,\n",
      "        -0.24921475, -0.1948112 , -0.19097015,  0.2794209 ,  0.01780919,\n",
      "         0.20495333,  0.0095434 , -0.12963448,  0.30371854,  0.10006017,\n",
      "        -0.27776176, -0.10968605, -0.17811263, -0.1433618 , -0.12162988,\n",
      "        -0.04110182,  0.07706167,  0.05049568, -0.17875701, -0.3860328 ,\n",
      "         0.00505842, -0.07291196, -0.40722618, -0.2156284 ,  0.59235203,\n",
      "        -0.10897525, -0.18519719,  0.28934336,  0.19387358, -0.47636503,\n",
      "         0.00747395,  0.30338395, -0.09336103, -0.04342851,  0.47588977,\n",
      "        -0.30900022,  0.36815625,  0.2563057 ,  0.06379744, -0.4346934 ,\n",
      "         0.12503102, -0.17250131,  0.03874268, -0.21635582,  0.08271933,\n",
      "        -0.17173694, -0.25641367,  0.19416445,  0.20860513,  0.03182077,\n",
      "         0.35642928, -0.28067616,  0.23416221,  0.0495875 , -0.03945724,\n",
      "        -0.41843057, -0.20289433,  0.15317623, -0.02736725, -0.18415062,\n",
      "         0.01463948,  0.06228272,  0.09871286,  0.03875626,  0.18630323,\n",
      "        -0.16939987, -0.34956038, -0.18299773,  0.3677375 , -0.15701938,\n",
      "         0.09789745, -0.25128666,  0.44280076,  0.07680929, -0.11616097,\n",
      "        -0.13699691,  0.03913172, -0.2266762 ,  0.21976061, -0.01310172,\n",
      "         0.03350441, -0.09195612, -0.19939734,  0.23617417, -0.0193704 ],\n",
      "       [ 0.01347926,  0.20055184,  0.31838125, -0.32484505, -0.41532513,\n",
      "        -0.5708244 ,  0.18070224, -0.01395731,  0.08898736,  0.06536194,\n",
      "        -0.1118997 ,  0.11501613, -0.3659843 ,  0.15483281, -0.00062986,\n",
      "         0.07990106,  0.1159934 ,  0.18798329,  0.34890935,  0.1993974 ,\n",
      "         0.3847915 , -0.40783098,  0.0024493 ,  0.47671136,  0.30371538,\n",
      "        -0.0732741 , -0.2954699 ,  0.01081836, -0.2950072 ,  0.07195088,\n",
      "        -0.32210115,  0.03353067, -0.45092034,  0.11472458, -0.1914598 ,\n",
      "        -0.3831546 ,  0.2752785 ,  0.10530565, -0.00980827,  0.18623504,\n",
      "        -0.02557625,  0.09304194,  0.31528437,  0.0251507 , -0.01045297,\n",
      "        -0.36971045,  0.00424838, -0.06077561,  0.0780465 ,  0.09031022,\n",
      "        -0.17071939,  0.26903775,  0.42084083, -0.02831455, -0.32573786,\n",
      "         0.19837897, -0.28789085, -0.37789002,  0.00790782, -0.39183637,\n",
      "         0.14340037,  0.1673975 ,  0.21806648, -0.03071696,  0.0148173 ,\n",
      "         0.05839533, -0.18954664,  0.36677662,  0.36949688,  0.17235427,\n",
      "        -0.34027117, -0.24118176, -0.49270812, -0.01093536,  0.2317366 ,\n",
      "        -0.48933637,  0.07281781,  0.1345757 , -0.4551851 ,  0.2783755 ,\n",
      "         0.16136655, -0.24508902,  0.02515225,  0.16853572,  0.15584937,\n",
      "         0.28321752,  0.23659265,  0.13919668,  0.02062   ,  0.11751173,\n",
      "         0.1411398 , -0.18081601, -0.09249479,  0.4132658 , -0.33953422,\n",
      "         0.2094077 , -0.23643997,  0.16665997,  0.08283585,  0.09171981]],\n",
      "      dtype=float32), array([ 0.2912848 ,  0.07637908,  0.00114704, -0.07664237, -0.04549833,\n",
      "       -0.11142594, -0.11876703, -0.0153367 , -0.03980493, -0.13299435,\n",
      "       -0.21585657,  0.00173317, -0.05512494, -0.00839909,  0.0184218 ,\n",
      "       -0.14853537, -0.11398143, -0.12904559,  0.01086172,  0.13823488,\n",
      "       -0.00051143, -0.06358696,  0.19151369, -0.03841164,  0.08757314,\n",
      "        0.08472489, -0.12911554,  0.23624526, -0.20395257, -0.10363336,\n",
      "       -0.0678802 ,  0.34287784, -0.12583026, -0.10889313, -0.11509989,\n",
      "       -0.11511271, -0.0414179 , -0.17441945,  0.33243847, -0.10386311,\n",
      "        0.29949892, -0.10633661, -0.05849246,  0.03045482,  0.3091543 ,\n",
      "       -0.02342817,  0.04420626,  0.06567574, -0.013356  , -0.16210674,\n",
      "       -0.0151829 ,  0.13813315,  0.07369695,  0.02014439, -0.13155854,\n",
      "        0.16390936, -0.0703039 , -0.03310389, -0.12541455, -0.05149557,\n",
      "       -0.12423289, -0.16105439, -0.0210207 , -0.0678536 ,  0.3340488 ,\n",
      "        0.06503417, -0.10998147,  0.08336386,  0.09068038, -0.02861179,\n",
      "       -0.09726755, -0.06925841, -0.02118919,  0.11304404, -0.06832008,\n",
      "       -0.06808154,  0.2876631 , -0.0372132 , -0.12622076,  0.00739459,\n",
      "       -0.05136946, -0.03111523,  0.28331366, -0.02607399, -0.11403211,\n",
      "        0.00335179, -0.15544121, -0.01146859,  0.3513511 , -0.08290691,\n",
      "       -0.10604807, -0.1058706 , -0.1564383 ,  0.03312808, -0.14640136,\n",
      "       -0.1160991 , -0.1291903 , -0.12914065,  0.07087778,  0.05110329],\n",
      "      dtype=float32)]\n",
      "[]\n",
      "[array([[-0.23526591, -0.30778307, -0.06298847, ..., -0.01992127,\n",
      "        -0.13022065,  0.25041515],\n",
      "       [ 0.05168191, -0.11690136, -0.07950036, ..., -0.14498089,\n",
      "         0.10499705, -0.02995464],\n",
      "       [-0.08795998, -0.01705406,  0.13748862, ..., -0.04985341,\n",
      "         0.13087867,  0.1045721 ],\n",
      "       ...,\n",
      "       [-0.01949078, -0.0003888 , -0.0571074 , ..., -0.02881314,\n",
      "        -0.14450353,  0.00736665],\n",
      "       [-0.09482508, -0.18170846,  0.0560832 , ...,  0.09330059,\n",
      "        -0.06075065,  0.00883378],\n",
      "       [-0.1365845 , -0.13590652, -0.16768314, ..., -0.15467514,\n",
      "         0.00784094, -0.08908033]], dtype=float32), array([-0.07696968, -0.10322511, -0.12423827, -0.16278493, -0.09433288,\n",
      "       -0.18575777,  0.13284364,  0.12993708, -0.09012806,  0.14721444,\n",
      "       -0.06168258,  0.09124401, -0.1907992 ,  0.00869395, -0.09115384,\n",
      "        0.1456122 ,  0.08030984,  0.00155211, -0.0477687 , -0.1036939 ,\n",
      "       -0.13392907, -0.08189888, -0.0481466 , -0.14121172,  0.12756763,\n",
      "       -0.16513121,  0.14405909, -0.17511219, -0.11378355, -0.13238111,\n",
      "       -0.05295928, -0.17910394,  0.12892978, -0.05383204, -0.17005324,\n",
      "        0.09936233,  0.04441595, -0.15496984,  0.14939538,  0.18183537,\n",
      "       -0.12452305, -0.12063392,  0.10394906, -0.07002598, -0.11774918,\n",
      "       -0.09174085, -0.11752418, -0.10283065, -0.01567022,  0.16054606,\n",
      "       -0.1476189 ,  0.13291039, -0.09948494,  0.05765744, -0.0445435 ,\n",
      "       -0.06965838, -0.02228563, -0.11737304, -0.08795074,  0.08583512,\n",
      "        0.08333919, -0.18740343, -0.06949158,  0.1378613 , -0.18286793,\n",
      "       -0.14334257, -0.0978466 , -0.03742016,  0.14472412, -0.02939129,\n",
      "        0.11528146, -0.05410779, -0.12505265,  0.27960214, -0.16607071,\n",
      "       -0.11599235, -0.16093925,  0.09101009, -0.04759488,  0.06625538,\n",
      "        0.0740184 ,  0.04261698,  0.12069603, -0.15149671,  0.09492365,\n",
      "        0.00440193, -0.12029904,  0.01490026, -0.11485528,  0.13415319,\n",
      "       -0.08463127, -0.05904669,  0.1059583 , -0.06429435,  0.14026147,\n",
      "        0.11795704, -0.13316734,  0.05752308,  0.15504208,  0.05868343],\n",
      "      dtype=float32)]\n",
      "[]\n",
      "[array([[-0.13983536, -0.10591239],\n",
      "       [-0.25409564, -0.16691774],\n",
      "       [-0.1117268 ,  0.18271603],\n",
      "       [-0.42023665,  0.14567494],\n",
      "       [-0.17882869, -0.08859164],\n",
      "       [-0.61980176,  0.11558576],\n",
      "       [ 0.24500401, -0.26390114],\n",
      "       [ 0.13596053, -0.14526519],\n",
      "       [-0.01392685,  0.6724736 ],\n",
      "       [ 0.22196156, -0.2472974 ],\n",
      "       [ 0.06459167,  0.4576165 ],\n",
      "       [ 0.12484124, -0.03569439],\n",
      "       [-0.55692214, -0.07412762],\n",
      "       [ 0.07199141,  0.03291499],\n",
      "       [-0.071319  ,  0.25268596],\n",
      "       [ 0.2490784 , -0.22443192],\n",
      "       [ 0.2789652 ,  0.03516309],\n",
      "       [ 0.13433258,  0.01149838],\n",
      "       [ 0.14961383,  0.11470103],\n",
      "       [-0.07884903,  0.10577122],\n",
      "       [-0.12004117,  0.21383385],\n",
      "       [-0.13953489, -0.15794052],\n",
      "       [-0.11819762, -0.11675873],\n",
      "       [-0.47847807,  0.00746073],\n",
      "       [ 0.05461056, -0.19132358],\n",
      "       [-0.2724901 ,  0.19417194],\n",
      "       [ 0.20185678, -0.17021663],\n",
      "       [-0.8552734 ,  0.07515237],\n",
      "       [-0.29706094,  0.07550135],\n",
      "       [-0.14348744,  0.24753344],\n",
      "       [ 0.01941811,  0.39826366],\n",
      "       [-0.8797826 ,  0.04286287],\n",
      "       [ 0.18674554, -0.13326037],\n",
      "       [ 0.19497235,  0.39530468],\n",
      "       [-0.63970923,  0.10071875],\n",
      "       [ 0.0273096 , -0.1780982 ],\n",
      "       [ 0.09922517,  0.02498372],\n",
      "       [-0.2798152 ,  0.24978264],\n",
      "       [-0.05667337, -0.18502052],\n",
      "       [ 0.93505   ,  0.39231455],\n",
      "       [-0.40308842,  0.44787604],\n",
      "       [-0.3749024 , -0.11140092],\n",
      "       [ 0.20865169, -0.0751631 ],\n",
      "       [ 0.05042722,  0.38854256],\n",
      "       [-0.43025517, -0.16812675],\n",
      "       [-0.08892671,  0.5467266 ],\n",
      "       [-0.20620593,  0.31155655],\n",
      "       [-0.35739237, -0.05472967],\n",
      "       [ 0.18160313,  0.13406652],\n",
      "       [ 0.13243842, -0.16034615],\n",
      "       [-0.23331156,  0.1655654 ],\n",
      "       [ 0.15340814, -0.18511175],\n",
      "       [-0.43724233,  0.08355268],\n",
      "       [ 0.14717875,  0.00960194],\n",
      "       [ 0.07145391,  0.05520809],\n",
      "       [-0.02547348,  0.339886  ],\n",
      "       [ 0.12946191,  0.21710753],\n",
      "       [-0.22589993,  0.27365246],\n",
      "       [ 0.09516335,  0.12104329],\n",
      "       [ 0.076828  , -0.0974145 ],\n",
      "       [ 0.31116414,  0.04534179],\n",
      "       [-0.74858505,  0.02368399],\n",
      "       [ 0.04962512,  0.17176458],\n",
      "       [ 0.25880787, -0.14094217],\n",
      "       [-0.2861644 ,  0.07161631],\n",
      "       [-0.26034087,  0.2809814 ],\n",
      "       [-0.09897672,  0.82201827],\n",
      "       [ 0.10361423,  0.24021491],\n",
      "       [ 0.29097542, -0.20891014],\n",
      "       [ 0.03503575,  0.31706488],\n",
      "       [ 0.1421134 , -0.13465703],\n",
      "       [ 0.05834409,  0.5574284 ],\n",
      "       [-0.31267607,  0.00887926],\n",
      "       [ 0.29671595, -0.746625  ],\n",
      "       [-0.5598137 ,  0.01197447],\n",
      "       [ 0.03289559,  0.56854755],\n",
      "       [-0.43903252,  0.14062165],\n",
      "       [ 0.06907995, -0.14204709],\n",
      "       [ 0.07732411,  0.8798266 ],\n",
      "       [ 0.03754104, -0.0522517 ],\n",
      "       [ 0.07863838, -0.07856257],\n",
      "       [ 0.1162618 ,  1.0033185 ],\n",
      "       [ 0.06379921, -0.09205695],\n",
      "       [-0.31084615,  0.06808467],\n",
      "       [ 0.02736269, -0.04650629],\n",
      "       [ 0.15012246,  0.7855845 ],\n",
      "       [ 0.01729464,  0.39075926],\n",
      "       [-0.12480023, -0.04029111],\n",
      "       [-0.04062641,  0.3288818 ],\n",
      "       [ 0.14611946, -0.14395665],\n",
      "       [ 0.04829526,  0.18446231],\n",
      "       [ 0.04053393,  0.23365791],\n",
      "       [ 0.03514965, -0.05591575],\n",
      "       [ 0.04913826,  0.5294394 ],\n",
      "       [ 0.25442645, -0.1744924 ],\n",
      "       [ 0.2335803 , -0.11045667],\n",
      "       [-0.4973256 ,  0.00789155],\n",
      "       [ 0.14254475,  0.02389337],\n",
      "       [ 0.19887042, -0.1650295 ],\n",
      "       [ 0.19856401,  0.00762099]], dtype=float32), array([ 0.11678404, -0.07732227], dtype=float32)]\n",
      "[array([4.4013023, 4.352894 ], dtype=float32), array([1., 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0310314  0.96896863]\n",
      " [0.03103757 0.96896243]\n",
      " [0.03116977 0.9688302 ]\n",
      " [0.03126318 0.9687368 ]\n",
      " [0.03103203 0.968968  ]\n",
      " [0.03235485 0.96764517]\n",
      " [0.0311416  0.96885836]\n",
      " [0.9677999  0.03220012]\n",
      " [0.04832064 0.9516793 ]\n",
      " [0.96785665 0.03214335]\n",
      " [0.03100463 0.9689954 ]\n",
      " [0.9681928  0.03180718]\n",
      " [0.03857282 0.9614272 ]\n",
      " [0.9680337  0.03196637]\n",
      " [0.9681271  0.03187288]\n",
      " [0.03114793 0.96885204]\n",
      " [0.03094903 0.96905094]\n",
      " [0.03109645 0.96890354]\n",
      " [0.03287277 0.9671272 ]\n",
      " [0.03091244 0.96908754]\n",
      " [0.9667097  0.03329033]\n",
      " [0.9668274  0.03317257]\n",
      " [0.03319215 0.96680784]\n",
      " [0.968058   0.03194201]\n",
      " [0.03204497 0.96795505]\n",
      " [0.9683155  0.03168453]\n",
      " [0.9680177  0.03198227]\n",
      " [0.03092121 0.9690788 ]\n",
      " [0.9578015  0.04219851]\n",
      " [0.96771044 0.03228956]\n",
      " [0.9681219  0.03187807]\n",
      " [0.03120007 0.96879995]\n",
      " [0.03103432 0.9689657 ]\n",
      " [0.03115054 0.9688495 ]\n",
      " [0.03128567 0.96871436]\n",
      " [0.96721625 0.03278375]\n",
      " [0.0312144  0.9687856 ]\n",
      " [0.968136   0.03186391]\n",
      " [0.9680297  0.03197029]\n",
      " [0.9668019  0.03319816]\n",
      " [0.03110458 0.96889544]\n",
      " [0.03091062 0.9690894 ]\n",
      " [0.0312797  0.9687203 ]\n",
      " [0.9666201  0.03337992]\n",
      " [0.9677414  0.03225869]\n",
      " [0.96688277 0.0331172 ]\n",
      " [0.9679349  0.03206513]\n",
      " [0.96752876 0.03247125]\n",
      " [0.96784484 0.0321552 ]\n",
      " [0.9680416  0.03195843]\n",
      " [0.03297308 0.96702695]\n",
      " [0.9675173  0.03248266]\n",
      " [0.96724045 0.03275954]\n",
      " [0.96726394 0.03273606]\n",
      " [0.9662434  0.03375663]\n",
      " [0.03255904 0.96744096]\n",
      " [0.9680332  0.03196686]\n",
      " [0.9666578  0.03334219]\n",
      " [0.03540146 0.96459854]\n",
      " [0.9683273  0.03167278]\n",
      " [0.9679883  0.03201173]\n",
      " [0.03104517 0.9689548 ]\n",
      " [0.9675728  0.03242725]\n",
      " [0.9680799  0.03192001]\n",
      " [0.03224874 0.96775126]\n",
      " [0.03117589 0.9688241 ]\n",
      " [0.03144446 0.9685555 ]\n",
      " [0.96793944 0.03206056]\n",
      " [0.9679319  0.03206807]\n",
      " [0.03096038 0.9690396 ]\n",
      " [0.9683381  0.0316619 ]\n",
      " [0.03160369 0.9683963 ]\n",
      " [0.03092398 0.96907604]\n",
      " [0.96827483 0.03172511]\n",
      " [0.03091178 0.9690882 ]\n",
      " [0.968237   0.03176304]\n",
      " [0.967941   0.03205904]\n",
      " [0.03163315 0.96836686]\n",
      " [0.03103622 0.9689638 ]\n",
      " [0.03147183 0.96852815]\n",
      " [0.9680115  0.03198851]\n",
      " [0.96819514 0.03180491]\n",
      " [0.96828514 0.03171489]\n",
      " [0.03104695 0.9689531 ]\n",
      " [0.03108831 0.9689117 ]\n",
      " [0.03181764 0.9681823 ]\n",
      " [0.9680613  0.03193869]\n",
      " [0.03117701 0.968823  ]\n",
      " [0.960703   0.03929697]\n",
      " [0.03135159 0.96864843]\n",
      " [0.03097087 0.9690291 ]\n",
      " [0.9679997  0.0320003 ]\n",
      " [0.03102333 0.9689767 ]\n",
      " [0.03101783 0.96898216]\n",
      " [0.03285362 0.96714634]\n",
      " [0.03272729 0.96727276]\n",
      " [0.9666871  0.03331289]\n",
      " [0.9675873  0.03241269]\n",
      " [0.05642445 0.9435755 ]\n",
      " [0.03191464 0.96808535]\n",
      " [0.96831745 0.03168258]\n",
      " [0.9680928  0.03190717]\n",
      " [0.03098013 0.9690199 ]\n",
      " [0.9679285  0.03207145]\n",
      " [0.9675491  0.03245098]\n",
      " [0.03108599 0.96891403]\n",
      " [0.96813583 0.03186416]\n",
      " [0.9681454  0.03185458]\n",
      " [0.03112554 0.96887445]\n",
      " [0.96815985 0.03184013]\n",
      " [0.03095869 0.9690413 ]\n",
      " [0.03133181 0.9686682 ]\n",
      " [0.96752197 0.03247805]\n",
      " [0.03118759 0.9688124 ]\n",
      " [0.96796715 0.03203277]\n",
      " [0.9683284  0.03167161]\n",
      " [0.9681543  0.03184574]\n",
      " [0.9666985  0.03330151]\n",
      " [0.9661946  0.0338053 ]\n",
      " [0.9679426  0.03205738]\n",
      " [0.03119081 0.9688092 ]\n",
      " [0.9609115  0.03908847]\n",
      " [0.03100366 0.96899635]\n",
      " [0.03129628 0.9687037 ]\n",
      " [0.03103093 0.96896905]\n",
      " [0.9674667  0.03253333]\n",
      " [0.03107389 0.96892613]\n",
      " [0.03138897 0.968611  ]\n",
      " [0.9674451  0.03255491]\n",
      " [0.03098779 0.9690122 ]\n",
      " [0.9680863  0.0319137 ]\n",
      " [0.03205292 0.9679471 ]\n",
      " [0.03100743 0.9689926 ]\n",
      " [0.96743846 0.03256155]\n",
      " [0.03204135 0.9679586 ]\n",
      " [0.0312268  0.9687732 ]\n",
      " [0.03635886 0.96364117]\n",
      " [0.03118299 0.968817  ]\n",
      " [0.95677507 0.04322489]\n",
      " [0.9683392  0.03166084]\n",
      " [0.03093934 0.96906066]\n",
      " [0.96811724 0.0318827 ]\n",
      " [0.96703494 0.03296509]\n",
      " [0.9674661  0.03253394]\n",
      " [0.9672627  0.03273727]\n",
      " [0.968299   0.03170107]\n",
      " [0.03261577 0.9673842 ]\n",
      " [0.9681636  0.03183637]\n",
      " [0.9683118  0.03168818]\n",
      " [0.03094369 0.9690563 ]\n",
      " [0.03109222 0.9689078 ]\n",
      " [0.96798533 0.03201469]\n",
      " [0.96769536 0.03230459]\n",
      " [0.96807176 0.03192817]\n",
      " [0.96767694 0.03232308]\n",
      " [0.03114691 0.9688531 ]\n",
      " [0.03098336 0.9690166 ]\n",
      " [0.9672117  0.03278827]\n",
      " [0.96532595 0.03467406]\n",
      " [0.03143393 0.96856606]\n",
      " [0.96789944 0.03210057]\n",
      " [0.03097977 0.96902025]\n",
      " [0.03159796 0.968402  ]\n",
      " [0.03163049 0.96836954]\n",
      " [0.9682009  0.03179906]\n",
      " [0.96691823 0.03308176]\n",
      " [0.03106829 0.96893173]\n",
      " [0.9674275  0.03257243]\n",
      " [0.03095762 0.96904236]\n",
      " [0.03316727 0.96683276]\n",
      " [0.03306693 0.9669331 ]\n",
      " [0.96446276 0.03553728]\n",
      " [0.9675829  0.03241714]\n",
      " [0.968335   0.03166506]\n",
      " [0.03094252 0.9690575 ]\n",
      " [0.96339124 0.03660878]\n",
      " [0.03125993 0.96874005]\n",
      " [0.96788573 0.0321143 ]\n",
      " [0.96821696 0.03178307]\n",
      " [0.96808517 0.03191481]\n",
      " [0.96772975 0.03227024]\n",
      " [0.03353488 0.9664652 ]\n",
      " [0.03915465 0.96084535]\n",
      " [0.03105739 0.9689426 ]\n",
      " [0.96664745 0.03335252]\n",
      " [0.03117614 0.96882385]\n",
      " [0.9682714  0.03172855]\n",
      " [0.03173554 0.9682644 ]\n",
      " [0.9681912  0.03180876]\n",
      " [0.967361   0.03263905]\n",
      " [0.96814716 0.03185282]\n",
      " [0.96508056 0.03491941]\n",
      " [0.9671295  0.03287051]\n",
      " [0.9638548  0.03614519]\n",
      " [0.9677639  0.03223616]\n",
      " [0.03106092 0.9689391 ]\n",
      " [0.03121387 0.9687861 ]\n",
      " [0.03107754 0.9689225 ]\n",
      " [0.03113129 0.96886873]\n",
      " [0.03098603 0.969014  ]\n",
      " [0.03114633 0.96885365]\n",
      " [0.03137933 0.96862066]\n",
      " [0.9680406  0.03195942]\n",
      " [0.03091453 0.96908545]\n",
      " [0.96829563 0.03170434]\n",
      " [0.03129219 0.9687078 ]\n",
      " [0.9679832  0.03201685]\n",
      " [0.9675561  0.03244384]\n",
      " [0.967398   0.03260201]\n",
      " [0.9679869  0.03201307]\n",
      " [0.03135306 0.96864694]\n",
      " [0.03123994 0.9687601 ]\n",
      " [0.03114411 0.96885586]\n",
      " [0.9681262  0.0318738 ]\n",
      " [0.9679717  0.03202832]\n",
      " [0.03098687 0.96901315]\n",
      " [0.03108241 0.9689176 ]\n",
      " [0.0312477  0.9687523 ]\n",
      " [0.03114735 0.96885264]\n",
      " [0.96764255 0.03235744]\n",
      " [0.03131351 0.9686865 ]\n",
      " [0.03102109 0.96897894]\n",
      " [0.03102325 0.96897674]\n",
      " [0.03097807 0.9690219 ]\n",
      " [0.967541   0.03245908]\n",
      " [0.06895877 0.9310412 ]\n",
      " [0.03110706 0.968893  ]\n",
      " [0.03264974 0.9673503 ]\n",
      " [0.03127038 0.9687296 ]\n",
      " [0.03105146 0.96894854]\n",
      " [0.03254579 0.9674542 ]\n",
      " [0.96732885 0.03267115]\n",
      " [0.9680873  0.03191273]\n",
      " [0.9681442  0.03185578]\n",
      " [0.03486902 0.965131  ]\n",
      " [0.03102367 0.9689763 ]\n",
      " [0.03164361 0.96835643]\n",
      " [0.9678337  0.03216628]\n",
      " [0.03101643 0.9689836 ]\n",
      " [0.03115918 0.9688408 ]\n",
      " [0.9666776  0.03332241]\n",
      " [0.03281765 0.96718234]\n",
      " [0.03092534 0.96907467]\n",
      " [0.96810144 0.03189852]\n",
      " [0.9669517  0.03304822]\n",
      " [0.03962869 0.9603713 ]\n",
      " [0.9681597  0.03184026]\n",
      " [0.96805364 0.03194633]\n",
      " [0.03109638 0.9689036 ]\n",
      " [0.96794707 0.03205293]\n",
      " [0.967577   0.03242303]\n",
      " [0.03156064 0.9684394 ]\n",
      " [0.96748966 0.03251031]\n",
      " [0.0317298  0.9682702 ]\n",
      " [0.9676174  0.03238258]\n",
      " [0.96524364 0.03475636]\n",
      " [0.96791863 0.03208138]\n",
      " [0.96815467 0.03184531]\n",
      " [0.96807575 0.03192429]\n",
      " [0.96771026 0.03228977]\n",
      " [0.03168874 0.96831125]\n",
      " [0.9683173  0.03168271]\n",
      " [0.03127301 0.968727  ]\n",
      " [0.9680965  0.03190351]\n",
      " [0.9674583  0.03254171]\n",
      " [0.9677153  0.03228467]\n",
      " [0.0313133  0.9686867 ]\n",
      " [0.9678506  0.03214938]\n",
      " [0.9678987  0.03210128]\n",
      " [0.9660601  0.03393986]\n",
      " [0.9681055  0.03189447]\n",
      " [0.96783185 0.03216815]\n",
      " [0.96714073 0.0328593 ]\n",
      " [0.9680146  0.03198543]\n",
      " [0.9679453  0.03205475]\n",
      " [0.96780634 0.03219373]\n",
      " [0.96805936 0.03194072]\n",
      " [0.03131676 0.96868324]\n",
      " [0.967924   0.03207605]\n",
      " [0.03176472 0.96823525]\n",
      " [0.03093678 0.9690632 ]\n",
      " [0.96711063 0.03288943]\n",
      " [0.032644   0.967356  ]\n",
      " [0.03093843 0.96906155]\n",
      " [0.9668331  0.0331669 ]\n",
      " [0.96797085 0.03202918]\n",
      " [0.96799314 0.03200692]\n",
      " [0.03128989 0.96871006]\n",
      " [0.96748453 0.0325155 ]\n",
      " [0.03200723 0.9679928 ]\n",
      " [0.03111741 0.9688826 ]\n",
      " [0.96815795 0.03184206]\n",
      " [0.03117655 0.96882343]\n",
      " [0.03196868 0.96803135]\n",
      " [0.03115982 0.9688402 ]\n",
      " [0.96700484 0.03299518]\n",
      " [0.9680496  0.03195039]\n",
      " [0.03204381 0.9679562 ]\n",
      " [0.03091962 0.9690804 ]\n",
      " [0.03112226 0.96887773]\n",
      " [0.03307731 0.9669227 ]\n",
      " [0.0325764  0.9674236 ]\n",
      " [0.03135148 0.9686485 ]\n",
      " [0.03094668 0.9690533 ]\n",
      " [0.96819705 0.03180294]\n",
      " [0.96587753 0.03412247]\n",
      " [0.9681671  0.03183288]\n",
      " [0.03094162 0.9690584 ]\n",
      " [0.9676061  0.03239387]\n",
      " [0.03091164 0.9690884 ]\n",
      " [0.03092889 0.9690711 ]\n",
      " [0.03138784 0.9686122 ]\n",
      " [0.0313205  0.96867955]\n",
      " [0.03105216 0.9689478 ]\n",
      " [0.03093838 0.9690616 ]\n",
      " [0.9679998  0.03200014]\n",
      " [0.03581509 0.96418494]\n",
      " [0.9679989  0.03200105]\n",
      " [0.03145858 0.96854144]\n",
      " [0.96794856 0.03205144]\n",
      " [0.0309971  0.9690029 ]\n",
      " [0.03104579 0.9689542 ]\n",
      " [0.03091452 0.96908545]\n",
      " [0.96649337 0.03350666]\n",
      " [0.9681488  0.0318512 ]\n",
      " [0.03151207 0.9684879 ]\n",
      " [0.03260551 0.96739453]\n",
      " [0.96810377 0.03189621]\n",
      " [0.9673753  0.03262472]\n",
      " [0.0310185  0.9689815 ]\n",
      " [0.03109411 0.96890587]\n",
      " [0.03097926 0.9690207 ]\n",
      " [0.9681082  0.0318919 ]\n",
      " [0.03091072 0.96908927]\n",
      " [0.9681992  0.03180077]\n",
      " [0.9676737  0.03232624]\n",
      " [0.0310482  0.9689518 ]\n",
      " [0.96796227 0.03203773]\n",
      " [0.96493685 0.03506314]\n",
      " [0.03138122 0.96861875]\n",
      " [0.03101042 0.96898955]\n",
      " [0.03102727 0.96897274]\n",
      " [0.03137349 0.9686265 ]\n",
      " [0.96760184 0.03239821]\n",
      " [0.9080451  0.09195496]\n",
      " [0.96773034 0.03226969]\n",
      " [0.03125557 0.9687444 ]\n",
      " [0.03118924 0.96881074]\n",
      " [0.967393   0.03260698]\n",
      " [0.9679193  0.03208074]\n",
      " [0.96675086 0.03324912]\n",
      " [0.03111024 0.9688898 ]\n",
      " [0.03102686 0.9689731 ]\n",
      " [0.9680804  0.03191963]\n",
      " [0.03095332 0.9690467 ]\n",
      " [0.9668936  0.03310642]\n",
      " [0.9673041  0.03269584]\n",
      " [0.0309384  0.9690616 ]\n",
      " [0.9576876  0.04231243]\n",
      " [0.96806306 0.0319369 ]\n",
      " [0.9662512  0.03374879]\n",
      " [0.03155835 0.96844167]\n",
      " [0.96473265 0.03526732]\n",
      " [0.03151208 0.9684879 ]\n",
      " [0.968128   0.031872  ]\n",
      " [0.9675851  0.03241493]\n",
      " [0.96765023 0.03234977]\n",
      " [0.03118486 0.96881515]\n",
      " [0.03231328 0.9676867 ]\n",
      " [0.03124409 0.9687559 ]\n",
      " [0.0309106  0.9690894 ]\n",
      " [0.03137671 0.9686233 ]\n",
      " [0.03117487 0.96882516]\n",
      " [0.9677792  0.03222079]\n",
      " [0.96641415 0.03358587]\n",
      " [0.968085   0.03191502]\n",
      " [0.9677559  0.03224414]\n",
      " [0.04350191 0.9564981 ]\n",
      " [0.96739674 0.03260325]\n",
      " [0.03092776 0.9690722 ]\n",
      " [0.03150463 0.96849537]\n",
      " [0.9654459  0.03455414]\n",
      " [0.96092    0.03908006]\n",
      " [0.03132941 0.9686706 ]\n",
      " [0.03290508 0.9670949 ]\n",
      " [0.03252041 0.9674796 ]\n",
      " [0.9682821  0.0317179 ]\n",
      " [0.03322305 0.96677697]\n",
      " [0.9683275  0.03167247]\n",
      " [0.0372733  0.9627267 ]\n",
      " [0.03091764 0.96908236]\n",
      " [0.0311294  0.9688706 ]\n",
      " [0.96798694 0.03201304]\n",
      " [0.9672809  0.03271909]\n",
      " [0.03111169 0.96888834]\n",
      " [0.03122174 0.96877825]\n",
      " [0.03249824 0.96750176]\n",
      " [0.9681051  0.03189496]\n",
      " [0.96625686 0.03374311]\n",
      " [0.96679807 0.03320194]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.96896863\n",
      "1 0.96896243\n",
      "1 0.9688302\n",
      "1 0.9687368\n",
      "1 0.968968\n",
      "1 0.96764517\n",
      "1 0.96885836\n",
      "0 0.9677999\n",
      "1 0.9516793\n",
      "0 0.96785665\n",
      "1 0.9689954\n",
      "0 0.9681928\n",
      "1 0.9614272\n",
      "0 0.9680337\n",
      "0 0.9681271\n",
      "1 0.96885204\n",
      "1 0.96905094\n",
      "1 0.96890354\n",
      "1 0.9671272\n",
      "1 0.96908754\n",
      "0 0.9667097\n",
      "0 0.9668274\n",
      "1 0.96680784\n",
      "0 0.968058\n",
      "1 0.96795505\n",
      "0 0.9683155\n",
      "0 0.9680177\n",
      "1 0.9690788\n",
      "0 0.9578015\n",
      "0 0.96771044\n",
      "0 0.9681219\n",
      "1 0.96879995\n",
      "1 0.9689657\n",
      "1 0.9688495\n",
      "1 0.96871436\n",
      "0 0.96721625\n",
      "1 0.9687856\n",
      "0 0.968136\n",
      "0 0.9680297\n",
      "0 0.9668019\n",
      "1 0.96889544\n",
      "1 0.9690894\n",
      "1 0.9687203\n",
      "0 0.9666201\n",
      "0 0.9677414\n",
      "0 0.96688277\n",
      "0 0.9679349\n",
      "0 0.96752876\n",
      "0 0.96784484\n",
      "0 0.9680416\n",
      "1 0.96702695\n",
      "0 0.9675173\n",
      "0 0.96724045\n",
      "0 0.96726394\n",
      "0 0.9662434\n",
      "1 0.96744096\n",
      "0 0.9680332\n",
      "0 0.9666578\n",
      "1 0.96459854\n",
      "0 0.9683273\n",
      "0 0.9679883\n",
      "1 0.9689548\n",
      "0 0.9675728\n",
      "0 0.9680799\n",
      "1 0.96775126\n",
      "1 0.9688241\n",
      "1 0.9685555\n",
      "0 0.96793944\n",
      "0 0.9679319\n",
      "1 0.9690396\n",
      "0 0.9683381\n",
      "1 0.9683963\n",
      "1 0.96907604\n",
      "0 0.96827483\n",
      "1 0.9690882\n",
      "0 0.968237\n",
      "0 0.967941\n",
      "1 0.96836686\n",
      "1 0.9689638\n",
      "1 0.96852815\n",
      "0 0.9680115\n",
      "0 0.96819514\n",
      "0 0.96828514\n",
      "1 0.9689531\n",
      "1 0.9689117\n",
      "1 0.9681823\n",
      "0 0.9680613\n",
      "1 0.968823\n",
      "0 0.960703\n",
      "1 0.96864843\n",
      "1 0.9690291\n",
      "0 0.9679997\n",
      "1 0.9689767\n",
      "1 0.96898216\n",
      "1 0.96714634\n",
      "1 0.96727276\n",
      "0 0.9666871\n",
      "0 0.9675873\n",
      "1 0.9435755\n",
      "1 0.96808535\n",
      "0 0.96831745\n",
      "0 0.9680928\n",
      "1 0.9690199\n",
      "0 0.9679285\n",
      "0 0.9675491\n",
      "1 0.96891403\n",
      "0 0.96813583\n",
      "0 0.9681454\n",
      "1 0.96887445\n",
      "0 0.96815985\n",
      "1 0.9690413\n",
      "1 0.9686682\n",
      "0 0.96752197\n",
      "1 0.9688124\n",
      "0 0.96796715\n",
      "0 0.9683284\n",
      "0 0.9681543\n",
      "0 0.9666985\n",
      "0 0.9661946\n",
      "0 0.9679426\n",
      "1 0.9688092\n",
      "0 0.9609115\n",
      "1 0.96899635\n",
      "1 0.9687037\n",
      "1 0.96896905\n",
      "0 0.9674667\n",
      "1 0.96892613\n",
      "1 0.968611\n",
      "0 0.9674451\n",
      "1 0.9690122\n",
      "0 0.9680863\n",
      "1 0.9679471\n",
      "1 0.9689926\n",
      "0 0.96743846\n",
      "1 0.9679586\n",
      "1 0.9687732\n",
      "1 0.96364117\n",
      "1 0.968817\n",
      "0 0.95677507\n",
      "0 0.9683392\n",
      "1 0.96906066\n",
      "0 0.96811724\n",
      "0 0.96703494\n",
      "0 0.9674661\n",
      "0 0.9672627\n",
      "0 0.968299\n",
      "1 0.9673842\n",
      "0 0.9681636\n",
      "0 0.9683118\n",
      "1 0.9690563\n",
      "1 0.9689078\n",
      "0 0.96798533\n",
      "0 0.96769536\n",
      "0 0.96807176\n",
      "0 0.96767694\n",
      "1 0.9688531\n",
      "1 0.9690166\n",
      "0 0.9672117\n",
      "0 0.96532595\n",
      "1 0.96856606\n",
      "0 0.96789944\n",
      "1 0.96902025\n",
      "1 0.968402\n",
      "1 0.96836954\n",
      "0 0.9682009\n",
      "0 0.96691823\n",
      "1 0.96893173\n",
      "0 0.9674275\n",
      "1 0.96904236\n",
      "1 0.96683276\n",
      "1 0.9669331\n",
      "0 0.96446276\n",
      "0 0.9675829\n",
      "0 0.968335\n",
      "1 0.9690575\n",
      "0 0.96339124\n",
      "1 0.96874005\n",
      "0 0.96788573\n",
      "0 0.96821696\n",
      "0 0.96808517\n",
      "0 0.96772975\n",
      "1 0.9664652\n",
      "1 0.96084535\n",
      "1 0.9689426\n",
      "0 0.96664745\n",
      "1 0.96882385\n",
      "0 0.9682714\n",
      "1 0.9682644\n",
      "0 0.9681912\n",
      "0 0.967361\n",
      "0 0.96814716\n",
      "0 0.96508056\n",
      "0 0.9671295\n",
      "0 0.9638548\n",
      "0 0.9677639\n",
      "1 0.9689391\n",
      "1 0.9687861\n",
      "1 0.9689225\n",
      "1 0.96886873\n",
      "1 0.969014\n",
      "1 0.96885365\n",
      "1 0.96862066\n",
      "0 0.9680406\n",
      "1 0.96908545\n",
      "0 0.96829563\n",
      "1 0.9687078\n",
      "0 0.9679832\n",
      "0 0.9675561\n",
      "0 0.967398\n",
      "0 0.9679869\n",
      "1 0.96864694\n",
      "1 0.9687601\n",
      "1 0.96885586\n",
      "0 0.9681262\n",
      "0 0.9679717\n",
      "1 0.96901315\n",
      "1 0.9689176\n",
      "1 0.9687523\n",
      "1 0.96885264\n",
      "0 0.96764255\n",
      "1 0.9686865\n",
      "1 0.96897894\n",
      "1 0.96897674\n",
      "1 0.9690219\n",
      "0 0.967541\n",
      "1 0.9310412\n",
      "1 0.968893\n",
      "1 0.9673503\n",
      "1 0.9687296\n",
      "1 0.96894854\n",
      "1 0.9674542\n",
      "0 0.96732885\n",
      "0 0.9680873\n",
      "0 0.9681442\n",
      "1 0.965131\n",
      "1 0.9689763\n",
      "1 0.96835643\n",
      "0 0.9678337\n",
      "1 0.9689836\n",
      "1 0.9688408\n",
      "0 0.9666776\n",
      "1 0.96718234\n",
      "1 0.96907467\n",
      "0 0.96810144\n",
      "0 0.9669517\n",
      "1 0.9603713\n",
      "0 0.9681597\n",
      "0 0.96805364\n",
      "1 0.9689036\n",
      "0 0.96794707\n",
      "0 0.967577\n",
      "1 0.9684394\n",
      "0 0.96748966\n",
      "1 0.9682702\n",
      "0 0.9676174\n",
      "0 0.96524364\n",
      "0 0.96791863\n",
      "0 0.96815467\n",
      "0 0.96807575\n",
      "0 0.96771026\n",
      "1 0.96831125\n",
      "0 0.9683173\n",
      "1 0.968727\n",
      "0 0.9680965\n",
      "0 0.9674583\n",
      "0 0.9677153\n",
      "1 0.9686867\n",
      "0 0.9678506\n",
      "0 0.9678987\n",
      "0 0.9660601\n",
      "0 0.9681055\n",
      "0 0.96783185\n",
      "0 0.96714073\n",
      "0 0.9680146\n",
      "0 0.9679453\n",
      "0 0.96780634\n",
      "0 0.96805936\n",
      "1 0.96868324\n",
      "0 0.967924\n",
      "1 0.96823525\n",
      "1 0.9690632\n",
      "0 0.96711063\n",
      "1 0.967356\n",
      "1 0.96906155\n",
      "0 0.9668331\n",
      "0 0.96797085\n",
      "0 0.96799314\n",
      "1 0.96871006\n",
      "0 0.96748453\n",
      "1 0.9679928\n",
      "1 0.9688826\n",
      "0 0.96815795\n",
      "1 0.96882343\n",
      "1 0.96803135\n",
      "1 0.9688402\n",
      "0 0.96700484\n",
      "0 0.9680496\n",
      "1 0.9679562\n",
      "1 0.9690804\n",
      "1 0.96887773\n",
      "1 0.9669227\n",
      "1 0.9674236\n",
      "1 0.9686485\n",
      "1 0.9690533\n",
      "0 0.96819705\n",
      "0 0.96587753\n",
      "0 0.9681671\n",
      "1 0.9690584\n",
      "0 0.9676061\n",
      "1 0.9690884\n",
      "1 0.9690711\n",
      "1 0.9686122\n",
      "1 0.96867955\n",
      "1 0.9689478\n",
      "1 0.9690616\n",
      "0 0.9679998\n",
      "1 0.96418494\n",
      "0 0.9679989\n",
      "1 0.96854144\n",
      "0 0.96794856\n",
      "1 0.9690029\n",
      "1 0.9689542\n",
      "1 0.96908545\n",
      "0 0.96649337\n",
      "0 0.9681488\n",
      "1 0.9684879\n",
      "1 0.96739453\n",
      "0 0.96810377\n",
      "0 0.9673753\n",
      "1 0.9689815\n",
      "1 0.96890587\n",
      "1 0.9690207\n",
      "0 0.9681082\n",
      "1 0.96908927\n",
      "0 0.9681992\n",
      "0 0.9676737\n",
      "1 0.9689518\n",
      "0 0.96796227\n",
      "0 0.96493685\n",
      "1 0.96861875\n",
      "1 0.96898955\n",
      "1 0.96897274\n",
      "1 0.9686265\n",
      "0 0.96760184\n",
      "0 0.9080451\n",
      "0 0.96773034\n",
      "1 0.9687444\n",
      "1 0.96881074\n",
      "0 0.967393\n",
      "0 0.9679193\n",
      "0 0.96675086\n",
      "1 0.9688898\n",
      "1 0.9689731\n",
      "0 0.9680804\n",
      "1 0.9690467\n",
      "0 0.9668936\n",
      "0 0.9673041\n",
      "1 0.9690616\n",
      "0 0.9576876\n",
      "0 0.96806306\n",
      "0 0.9662512\n",
      "1 0.96844167\n",
      "0 0.96473265\n",
      "1 0.9684879\n",
      "0 0.968128\n",
      "0 0.9675851\n",
      "0 0.96765023\n",
      "1 0.96881515\n",
      "1 0.9676867\n",
      "1 0.9687559\n",
      "1 0.9690894\n",
      "1 0.9686233\n",
      "1 0.96882516\n",
      "0 0.9677792\n",
      "0 0.96641415\n",
      "0 0.968085\n",
      "0 0.9677559\n",
      "1 0.9564981\n",
      "0 0.96739674\n",
      "1 0.9690722\n",
      "1 0.96849537\n",
      "0 0.9654459\n",
      "0 0.96092\n",
      "1 0.9686706\n",
      "1 0.9670949\n",
      "1 0.9674796\n",
      "0 0.9682821\n",
      "1 0.96677697\n",
      "0 0.9683275\n",
      "1 0.9627267\n",
      "1 0.96908236\n",
      "1 0.9688706\n",
      "0 0.96798694\n",
      "0 0.9672809\n",
      "1 0.96888834\n",
      "1 0.96877825\n",
      "1 0.96750176\n",
      "0 0.9681051\n",
      "0 0.96625686\n",
      "0 0.96679807\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict(X_test)\n",
    "for prediction in probs:\n",
    "    print(prediction.argmax(-1), prediction[prediction.argmax(-1)])\n",
    "#print(probs.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
