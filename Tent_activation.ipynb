{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.constraints import Constraint\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "#this function just clips values to a range (elementwise)\n",
    "class Between(Constraint):\n",
    "    def __init__(self, min_value, max_value):\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def __call__(self, w):        \n",
    "        return K.clip(w, self.min_value, self.max_value)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'min_value': self.min_value,\n",
    "                'max_value': self.max_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras.layers import activations\n",
    "from keras.layers import initializers\n",
    "from keras.layers import regularizers\n",
    "from keras.layers import constraints\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from  keras.engine.base_layer import InputSpec\n",
    "from keras.utils.generic_utils import to_list\n",
    "\n",
    "class Tent(Layer):\n",
    "    \"\"\"Tent activation Unit.\n",
    "    It follows:\n",
    "    `f(x) =  max( 0, theta -|x|) ,\n",
    "\n",
    "    where `theta` is a learned array with the same shape as x.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as the input.\n",
    "    # Arguments\n",
    "        theta_initializer: initializer function for the weights.\n",
    "        theta_regularizer: L2 regularization strenth\n",
    "        theta_max: highest allowed value for theta (min value is set to 0.05)\n",
    "        shared_axes: the axes along which to share learnable\n",
    "            parameters for the activation function.\n",
    "            For example, if the incoming feature maps\n",
    "            are from a 2D convolution\n",
    "            with output shape `(batch, height, width, channels)`,\n",
    "            and you wish to share parameters across space\n",
    "            so that each filter only has one set of parameters,\n",
    "            set `shared_axes=[1, 2]`.\n",
    "    # References\n",
    "    https://arxiv.org/pdf/1908.02435.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 theta_regularizer=0.12,\n",
    "                 theta_max=1.0,\n",
    "                 shared_axes=None,\n",
    "                 **kwargs):\n",
    "        super(Tent, self).__init__(**kwargs)\n",
    "        self.supports_masking = True # Do not know what this does, just let it be\n",
    "        self.theta_initializer = initializers.Ones() #see article\n",
    "        self.theta_regularizer = regularizers.l2(theta_regularizer) # I interpreted \"weight decay\" as l2, not l1 \n",
    "        self.theta_constraint = Between(min_value=0.05,max_value=theta_max)\n",
    "\n",
    "        if shared_axes is None:\n",
    "            self.shared_axes = None\n",
    "        else:\n",
    "            self.shared_axes = to_list(shared_axes, allow_tuple=True)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        param_shape = list(input_shape[1:])\n",
    "        self.param_broadcast = [False] * len(param_shape)\n",
    "        if self.shared_axes is not None:\n",
    "            for i in self.shared_axes:\n",
    "                param_shape[i - 1] = 1\n",
    "                self.param_broadcast[i - 1] = True\n",
    "        self.theta = self.add_weight(shape=param_shape,\n",
    "                                     name='theta',\n",
    "                                     initializer=self.theta_initializer,\n",
    "                                     regularizer=self.theta_regularizer,\n",
    "                                     constraint=self.theta_constraint)\n",
    "        # Set input spec\n",
    "        axes = {}\n",
    "        if self.shared_axes:\n",
    "            for i in range(1, len(input_shape)):\n",
    "                if i not in self.shared_axes:\n",
    "                    axes[i] = input_shape[i]\n",
    "        self.input_spec = InputSpec(ndim=len(input_shape), axes=axes)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        pos = K.relu(self.theta - K.abs(inputs))\n",
    "        return pos \n",
    "\n",
    "    #TODO: what is this config for? Proabably should update this to add \"theta_max\"\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'theta_initializer': initializers.serialize(self.theta_initializer),\n",
    "            'theta_regularizer': regularizers.serialize(self.theta_regularizer),\n",
    "            'shared_axes': self.shared_axes\n",
    "        }\n",
    "        base_config = super(Tent, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "print(np.shape(x_train))\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13066062\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(x_train))\n",
    "print(np.max(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.2020 - acc: 0.9420 - val_loss: 0.0621 - val_acc: 0.9801\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0658 - acc: 0.9811 - val_loss: 0.0402 - val_acc: 0.9883\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0513 - acc: 0.9855 - val_loss: 0.0336 - val_acc: 0.9892\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0406 - acc: 0.9877 - val_loss: 0.0271 - val_acc: 0.9916\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0350 - acc: 0.9896 - val_loss: 0.0243 - val_acc: 0.9919\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0327 - acc: 0.9903 - val_loss: 0.0232 - val_acc: 0.9926\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0282 - acc: 0.9917 - val_loss: 0.0271 - val_acc: 0.9916\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.0246 - acc: 0.9924 - val_loss: 0.0403 - val_acc: 0.9884\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0241 - acc: 0.9924 - val_loss: 0.0191 - val_acc: 0.9940\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0215 - acc: 0.9937 - val_loss: 0.0265 - val_acc: 0.9919\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.0192 - acc: 0.9941 - val_loss: 0.0290 - val_acc: 0.9909\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0187 - acc: 0.9942 - val_loss: 0.0221 - val_acc: 0.9928\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0166 - acc: 0.9950 - val_loss: 0.0241 - val_acc: 0.9932\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0247 - val_acc: 0.9933\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0162 - acc: 0.9953 - val_loss: 0.0194 - val_acc: 0.9942\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0259 - val_acc: 0.9917\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0118 - acc: 0.9963 - val_loss: 0.0246 - val_acc: 0.9936\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0159 - acc: 0.9950 - val_loss: 0.0203 - val_acc: 0.9944\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0244 - val_acc: 0.9942\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0215 - val_acc: 0.9941\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0252 - val_acc: 0.9938\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0103 - acc: 0.9972 - val_loss: 0.0206 - val_acc: 0.9949\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0270 - val_acc: 0.9936\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0220 - val_acc: 0.9943\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0099 - acc: 0.9972 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0285 - val_acc: 0.9936\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0330 - val_acc: 0.9924\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0233 - val_acc: 0.9942\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0235 - val_acc: 0.9940\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0298 - val_acc: 0.9930\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0210 - val_acc: 0.9942\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0233 - val_acc: 0.9946\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0267 - val_acc: 0.9935\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0327 - val_acc: 0.9936\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0218 - val_acc: 0.9952\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0189 - val_acc: 0.9950\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0248 - val_acc: 0.9940\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.0057 - acc: 0.9985 - val_loss: 0.0225 - val_acc: 0.9947\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0268 - val_acc: 0.9943\n",
      "tent mean confidence on train images  0.9997014\n",
      "tent mean confidence on test images  0.9979911\n",
      "tent mean confidence on random matrix  0.9999995\n",
      "tent mean confidence on flat_gray img  0.99758744\n",
      "tent mean confidence on avg_img  0.8715608\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "#baseline \"MNIST_CNN\" model used by the article https://arxiv.org/pdf/1908.02435.pdf\n",
    "# TODO: maybe we should not realy on the TENT article as reference. We should pick a\n",
    "#       well known architecture (AlexNet, VGG) and use it with all the variations we try\n",
    "\n",
    "baseline = Sequential()\n",
    "#layer1\n",
    "baseline.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "baseline.add(BatchNormalization())\n",
    "baseline.add(Activation(\"relu\"))\n",
    "#layer2\n",
    "baseline.add(Conv2D(32, (3, 3)))\n",
    "baseline.add(BatchNormalization())\n",
    "baseline.add(Activation(\"relu\"))\n",
    "\n",
    "baseline.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#layer3\n",
    "baseline.add(Conv2D(64, (3, 3), padding='same'))\n",
    "baseline.add(BatchNormalization())\n",
    "baseline.add(Activation(\"relu\"))\n",
    "#layer4\n",
    "baseline.add(Conv2D(64, (3, 3)))\n",
    "baseline.add(BatchNormalization())\n",
    "baseline.add(Activation(\"relu\"))\n",
    "\n",
    "baseline.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "baseline.add(Flatten())\n",
    "\n",
    "#layer5\n",
    "baseline.add(Dense(200))\n",
    "baseline.add(BatchNormalization())\n",
    "baseline.add(Activation(\"relu\"))\n",
    "baseline.add(Dropout(0.5))\n",
    "#layer6\n",
    "baseline.add(Dense(200))\n",
    "baseline.add(BatchNormalization())\n",
    "baseline.add(Activation(\"relu\"))\n",
    "baseline.add(Dropout(0.5))\n",
    "\n",
    "baseline.add(Dense(10))\n",
    "\n",
    "baseline.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "baseline.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fit\n",
    "baseline.fit(x_train, y_train, batch_size=100, epochs=40, validation_data=(x_test, y_test), shuffle=True)\n",
    "\n",
    "#stats\n",
    "print(\"tent mean confidence on train images \",np.mean(np.max(baseline.predict(x_train),axis=1)))\n",
    "print(\"tent mean confidence on test images \",np.mean(np.max(baseline.predict(x_test),axis=1)))\n",
    "\n",
    "ran = np.random.random(size=(100,28,28,1))\n",
    "print(\"tent mean confidence on random matrix (should be uniform)\",np.mean(np.max(baseline.predict(ran),axis=1)))\n",
    "\n",
    "flat_gray=np.ones((1,28,28,1))*0.5\n",
    "print(\"tent mean confidence on flat_gray img (should be uniform)\",np.mean(np.max(baseline.predict(flat_gray),axis=1)))\n",
    "\n",
    "avg_img = np.mean(x_test, axis=0,keepdims=True)\n",
    "print(\"tent mean confidence on avg_img (should be uniform)\",np.mean(np.max(baseline.predict(avg_img),axis=1)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline predictions on random (are they all the same?) [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"baseline predictions on random (are they all the same?)\",np.argmax(baseline.predict(ran),axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next let's use TENT in all layers\n",
    "TODO: give it a better model name, not overwrite \"model\" each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "#model from article https://arxiv.org/pdf/1908.02435.pdf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Tent(shared_axes=[0,1,2])) # only one tent width value per layer \n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Tent(shared_axes=[0,1,2]))  # only one tent width value per layer\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Tent(shared_axes=[0,1,2]))  # only one tent width value per layer\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Tent(shared_axes=[0,1,2])) # only one tent width value per layer\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Tent(shared_axes=[0])) # only one tent width value per layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Tent(shared_axes=[0])) # only one tent width value per layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10)) # I suppose no TENT in here. It does not really learn with tent here\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1)\n",
      "[array([1.], dtype=float32)]\n",
      "(1, 1)\n",
      "[array([1.], dtype=float32)]\n",
      "should be random (/uniform):   [[0.09124071 0.1431169  0.1185334  0.10619537 0.1105161  0.03032977\n",
      "  0.03706808 0.17415461 0.15287882 0.03596622]]\n"
     ]
    }
   ],
   "source": [
    "#let's just visualize the initialized tent widhts (should be 1 value per layer, initialized to 1)\n",
    "for lay in model.layers:\n",
    "    if \"theta_initializer\" in lay.get_config():\n",
    "        print(np.shape(lay.get_weights()))\n",
    "        print(lay.get_weights())\n",
    "\n",
    "output = model.predict(np.expand_dims(np.random.random(x_train.shape[1:]),0))\n",
    "print(\"should be random (/uniform):  \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deepmind/anaconda3/envs/new_tf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 2.7767 - acc: 0.1765 - val_loss: 2.9480 - val_acc: 0.0919\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 2.4147 - acc: 0.2031 - val_loss: 2.9314 - val_acc: 0.0932\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 2.3128 - acc: 0.2059 - val_loss: 2.8842 - val_acc: 0.0999\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 2.2742 - acc: 0.2091 - val_loss: 2.2643 - val_acc: 0.2196\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 2.2458 - acc: 0.2100 - val_loss: 3.0949 - val_acc: 0.0726\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 2.2205 - acc: 0.2170 - val_loss: 2.1846 - val_acc: 0.2240\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 2.1874 - acc: 0.2271 - val_loss: 3.2640 - val_acc: 0.0662\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 2.1574 - acc: 0.2504 - val_loss: 2.3251 - val_acc: 0.2151\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 2.0620 - acc: 0.3103 - val_loss: 2.9810 - val_acc: 0.1329\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 1.7413 - acc: 0.4349 - val_loss: 2.5636 - val_acc: 0.2946\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 1.3328 - acc: 0.5990 - val_loss: 1.8232 - val_acc: 0.5403\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.9567 - acc: 0.7671 - val_loss: 0.6397 - val_acc: 0.8752\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.6658 - acc: 0.8743 - val_loss: 0.4086 - val_acc: 0.9405\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.5211 - acc: 0.9132 - val_loss: 0.3545 - val_acc: 0.9521\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.4357 - acc: 0.9325 - val_loss: 0.9946 - val_acc: 0.7594\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.3842 - acc: 0.9419 - val_loss: 0.6711 - val_acc: 0.8357\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.3482 - acc: 0.9474 - val_loss: 0.7091 - val_acc: 0.8194\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.3200 - acc: 0.9521 - val_loss: 0.5481 - val_acc: 0.8742\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.2929 - acc: 0.9547 - val_loss: 1.1271 - val_acc: 0.7159\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.2644 - acc: 0.9601 - val_loss: 0.5933 - val_acc: 0.8555\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.2408 - acc: 0.9645 - val_loss: 0.5886 - val_acc: 0.8514\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.2312 - acc: 0.9654 - val_loss: 0.1855 - val_acc: 0.9749\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.2157 - acc: 0.9666 - val_loss: 0.4045 - val_acc: 0.9058\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1980 - acc: 0.9698 - val_loss: 0.7641 - val_acc: 0.8077\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.1903 - acc: 0.9710 - val_loss: 0.1881 - val_acc: 0.9681\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.1766 - acc: 0.9730 - val_loss: 0.1360 - val_acc: 0.9799\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.1672 - acc: 0.9736 - val_loss: 0.6952 - val_acc: 0.8518\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.1558 - acc: 0.9753 - val_loss: 0.8460 - val_acc: 0.7924\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.1483 - acc: 0.9760 - val_loss: 0.1343 - val_acc: 0.9771\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.1389 - acc: 0.9787 - val_loss: 0.2956 - val_acc: 0.9367\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.1347 - acc: 0.9779 - val_loss: 0.1181 - val_acc: 0.9824\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.1245 - acc: 0.9793 - val_loss: 0.3122 - val_acc: 0.9211\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.1163 - acc: 0.9808 - val_loss: 0.3246 - val_acc: 0.9170\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.1093 - acc: 0.9818 - val_loss: 0.0876 - val_acc: 0.9851\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1103 - acc: 0.9809 - val_loss: 0.1120 - val_acc: 0.9788\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1059 - acc: 0.9819 - val_loss: 1.2891 - val_acc: 0.7219\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1038 - acc: 0.9818 - val_loss: 0.7209 - val_acc: 0.8368\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0997 - acc: 0.9823 - val_loss: 1.3566 - val_acc: 0.6958\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0957 - acc: 0.9837 - val_loss: 0.0852 - val_acc: 0.9859\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0927 - acc: 0.9836 - val_loss: 0.0801 - val_acc: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3e4604e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=100, epochs=40, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "[array([[[0.23019592]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[0.23353232]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[0.18351622]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[0.18637131]]], dtype=float32)]\n",
      "(1, 1)\n",
      "[array([0.19555807], dtype=float32)]\n",
      "(1, 1)\n",
      "[array([0.29713175], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#let's see the fitted width values - see article for what they got...\n",
    "# notice this depends heavily on the regularization \n",
    "for lay in model.layers:\n",
    "    if \"theta_initializer\" in lay.get_config():\n",
    "        print(np.shape(lay.get_weights()))\n",
    "        print(lay.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent mean confidence on train images (compare with accuracy) 0.9914855\n",
      "tent mean confidence on test images (compare with accuracy) 0.9906426\n",
      "tent mean confidence on random matrixes (should be uniform) 0.68727577\n",
      "tent prediction on random  [2 8 8 2 8 8 2 8 8 8 5 8 8 8 8 8 8 0 8 0 8 8 8 8 8 0 8 8 8 2 8 8 8 8 6 0 8\n",
      " 6 8 8 2 8 8 8 8 8 0 8 8 8 8 2 8 2 6 8 8 8 8 8 8 8 8 8 0 8 5 8 6 2 0 8 8 8\n",
      " 0 8 8 8 8 8 8 8 8 8 2 8 8 5 8 8 2 8 0 8 5 8 8 8 8 6]\n",
      "tent mean confidence on flat_gray img (should be uniform)  0.9890951\n",
      "tent mean confidence on avg_img (should be uniform?) 0.5540315\n",
      "\n",
      " the weights theta are\n",
      "(1, 1, 1, 1)\n",
      "[array([[[0.23019592]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(\"tent mean confidence on train images (compare with accuracy)\",np.mean(np.max(model.predict(x_train),axis=1)))\n",
    "print(\"tent mean confidence on test images (compare with accuracy)\",np.mean(np.max(model.predict(x_test),axis=1)))\n",
    "\n",
    "\n",
    "ran = np.random.random(size=(100,28,28,1))\n",
    "\n",
    "print(\"tent mean confidence on random matrixes (should be uniform)\",np.mean(np.max(model.predict(ran),axis=1)))\n",
    "print(\"tent prediction on random \",np.argmax(model.predict(ran),axis=1))\n",
    "\n",
    "\n",
    "flat_gray=np.ones((100,28,28,1))*0.5\n",
    "print(\"tent mean confidence on flat_gray img (should be uniform) \",np.mean(np.max(model.predict(flat_gray),axis=1)))\n",
    "\n",
    "avg_img = np.mean(x_test, axis=0,keepdims=True)\n",
    "#print(np.shape(avg_img))\n",
    "print(\"tent mean confidence on avg_img (should be uniform?)\",np.mean(np.max(model.predict(avg_img),axis=1)))\n",
    "print(\"\\n the weights theta are\")\n",
    "print(np.shape(model.layers[2].get_weights()))\n",
    "print(model.layers[2].get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "These above results are an improvement over baseline (lower confidence on random and on avg img) \n",
    "But if you train again the result varies a lot. Sometimes it's pretty mucht the same.\n",
    "In any case this improvement is not enough, I think - flat gray image fails!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it calibrated?\n",
    "TODO: the cell below was written in hurry, copy-past+modify method. not sure it is correct. please double-check\n",
    "\n",
    "TODO2: models were very good on val set, so I just added noise to it to generate uncertainty. Maybe there's some elegant way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENT model mean confidence 0.85127014\n",
      "TENT model accuracy on noisy test set: 0.4712\n",
      "baseline mean confidence 0.97339636\n",
      "baseline accuracy on noisy test set: 0.9514\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "noisy_test = x_test*0.8+np.random.random(np.shape(x_test))*0.2\n",
    "\n",
    "all_confidences = np.max(model.predict(noisy_test),axis=1)\n",
    "print(\"TENT model mean confidence\", np.mean(all_confidences))\n",
    "wasit_correct = np.argmax(y_test,axis=1)==np.argmax(model.predict(noisy_test),axis=1)\n",
    "print(\"TENT model accuracy on noisy test set:\", np.mean(wasit_correct))\n",
    "\n",
    "order = np.argsort(all_confidences)\n",
    "ordered_confidences = all_confidences[order]\n",
    "ordered_wasit = wasit_correct[order]\n",
    "\n",
    "mean_acc=[]\n",
    "last_idx=0\n",
    "for i in np.arange(0.16,1.01,0.01):\n",
    "    stop_idx = np.searchsorted(ordered_confidences,i)\n",
    "    mean_acc.append(np.mean(ordered_wasit[last_idx:stop_idx]))\n",
    "    last_idx=stop_idx\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0.16,1.01,0.01)+0.01,mean_acc,label=\"tent_in_all_layers\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "plt.ylabel(\"accruacy\")\n",
    "plt.xlabel(\"confidence in predicted class\")\n",
    "#plt.legend()\n",
    "\n",
    "#####################\n",
    "\n",
    "all_confidences = np.max(baseline.predict(noisy_test),axis=1)\n",
    "print(\"baseline mean confidence\", np.mean(all_confidences))\n",
    "wasit_correct = np.argmax(y_test,axis=1)==np.argmax(baseline.predict(noisy_test),axis=1)\n",
    "print(\"baseline accuracy on noisy test set:\", np.mean(wasit_correct))\n",
    "\n",
    "order = np.argsort(all_confidences)\n",
    "ordered_confidences = all_confidences[order]\n",
    "ordered_wasit = wasit_correct[order]\n",
    "\n",
    "mean_acc=[]\n",
    "last_idx=0\n",
    "for i in np.arange(0.16,1.01,0.01):\n",
    "    stop_idx = np.searchsorted(ordered_confidences,i)\n",
    "    mean_acc.append(np.mean(ordered_wasit[last_idx:stop_idx]))\n",
    "    last_idx=stop_idx\n",
    "    \n",
    "plt.plot(np.arange(0.16,1.01,0.01)+0.01,mean_acc,label=\"relu_baseline\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST+last layer tent only\n",
    "TODO: better name, othewirse we overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "#model from article https://arxiv.org/pdf/1908.02435.pdf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Tent(shared_axes=[0,1,2]))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Tent(shared_axes=[0,1,2]))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Tent(shared_axes=[0,1,2]))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Tent(shared_axes=[0,1,2]))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Tent(shared_axes=[0]))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Tent(shared_axes=[0]))\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "#Notice high theta needed here, otherwise it is impossibel to get high probabilities. \n",
    "# so I use low regularization+high max\n",
    "model.add(Tent(shared_axes=[0], theta_regularizer=0.001,theta_max=15.0)) \n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 1.5140 - acc: 0.7959 - val_loss: 1.1401 - val_acc: 0.9555\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.9318 - acc: 0.9703 - val_loss: 0.7296 - val_acc: 0.9878\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.6352 - acc: 0.9806 - val_loss: 0.5089 - val_acc: 0.9888\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.4447 - acc: 0.9856 - val_loss: 0.3648 - val_acc: 0.9896\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.3218 - acc: 0.9872 - val_loss: 0.2659 - val_acc: 0.9909\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.2395 - acc: 0.9884 - val_loss: 0.2117 - val_acc: 0.9873\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1828 - acc: 0.9899 - val_loss: 0.1486 - val_acc: 0.9933\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.1414 - acc: 0.9915 - val_loss: 0.1263 - val_acc: 0.9913\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.1133 - acc: 0.9914 - val_loss: 0.1025 - val_acc: 0.9903\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0931 - acc: 0.9916 - val_loss: 0.0909 - val_acc: 0.9901\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0782 - acc: 0.9926 - val_loss: 0.0712 - val_acc: 0.9922\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0640 - acc: 0.9937 - val_loss: 0.0627 - val_acc: 0.9920\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0603 - acc: 0.9927 - val_loss: 0.0557 - val_acc: 0.9918\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0475 - acc: 0.9941 - val_loss: 0.0430 - val_acc: 0.9941\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0430 - acc: 0.9940 - val_loss: 0.0499 - val_acc: 0.9914\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0377 - acc: 0.9944 - val_loss: 0.0413 - val_acc: 0.9935\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0341 - acc: 0.9949 - val_loss: 0.0365 - val_acc: 0.9941\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0322 - acc: 0.9950 - val_loss: 0.0412 - val_acc: 0.9920\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0292 - acc: 0.9952 - val_loss: 0.0376 - val_acc: 0.9930\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0280 - acc: 0.9953 - val_loss: 0.0352 - val_acc: 0.9937\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0266 - acc: 0.9958 - val_loss: 0.0336 - val_acc: 0.9939\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0259 - acc: 0.9957 - val_loss: 0.0395 - val_acc: 0.9922\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0263 - acc: 0.9953 - val_loss: 0.0366 - val_acc: 0.9924\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0258 - acc: 0.9954 - val_loss: 0.0334 - val_acc: 0.9940\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.0259 - acc: 0.9956 - val_loss: 0.0317 - val_acc: 0.9936\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0231 - acc: 0.9964 - val_loss: 0.0352 - val_acc: 0.9933\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.0242 - acc: 0.9958 - val_loss: 0.0331 - val_acc: 0.9936\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0229 - acc: 0.9963 - val_loss: 0.0348 - val_acc: 0.9936\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0239 - acc: 0.9960 - val_loss: 0.0376 - val_acc: 0.9930\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0225 - acc: 0.9964 - val_loss: 0.0348 - val_acc: 0.9936\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0215 - acc: 0.9966 - val_loss: 0.0355 - val_acc: 0.9926\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0209 - acc: 0.9968 - val_loss: 0.0396 - val_acc: 0.9926\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0254 - acc: 0.9957 - val_loss: 0.0384 - val_acc: 0.9925\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0247 - acc: 0.9954 - val_loss: 0.0327 - val_acc: 0.9942\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0236 - acc: 0.9956 - val_loss: 0.0309 - val_acc: 0.9947\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0210 - acc: 0.9965 - val_loss: 0.0348 - val_acc: 0.9936\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0256 - acc: 0.9959 - val_loss: 0.0361 - val_acc: 0.9936\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0243 - acc: 0.9960 - val_loss: 0.0376 - val_acc: 0.9934\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0203 - acc: 0.9969 - val_loss: 0.0338 - val_acc: 0.9939\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0193 - acc: 0.9973 - val_loss: 0.0329 - val_acc: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49a904e210>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=100, epochs=40, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent mean confidence on train images  0.99817365\n",
      "tent mean confidence on test images  0.9959659\n",
      "tent predicction on random  [8 8 8 8 8 8 8 8 8 8]\n",
      "tent mean confidence on random matrix  0.9991867\n",
      "tent confidence on flat_gray img  0.84640956\n",
      "tent pred flat_gray imgx  [1]\n",
      "tent mean confidence on avg_img  0.7120763\n",
      "\n",
      " the weights theta are\n",
      "(1, 1)\n",
      "[array([9.368581], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(\"tent mean confidence on train images \",np.mean(np.max(model.predict(x_train),axis=1)))\n",
    "print(\"tent mean confidence on test images \",np.mean(np.max(model.predict(x_test),axis=1)))\n",
    "\n",
    "ran = np.random.random(size=(10,28,28,1))\n",
    "print(\"tent prediction on random \",np.argmax(model.predict(ran),axis=1))\n",
    "print(\"tent mean confidence on random matrix (should be uniform)\",np.mean(np.max(model.predict(ran),axis=1)))\n",
    "\n",
    "flat_gray=np.ones((1,28,28,1))*0.5\n",
    "print(\"tent confidence on flat_gray img (should be uniform)\",np.mean(np.max(model.predict(flat_gray),axis=1)))\n",
    "print(\"tent pred flat_gray imgx \",np.argmax(model.predict(flat_gray),axis=1))\n",
    "\n",
    "avg_img = np.mean(x_test, axis=0,keepdims=True)\n",
    "#print(np.shape(avg_img))\n",
    "print(\"tent mean confidence on avg_img (should be uniform)\",np.mean(np.max(model.predict(avg_img),axis=1)))\n",
    "print(\"\\n the weights theta are\")\n",
    "print(np.shape(model.layers[-2].get_weights()))\n",
    "print(model.layers[-2].get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing activations in intermediate layers:\n",
    "(to make it meaningful, one should do for all models and average, visualize somehow (histogram?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Tent object at 0x7f49a7022850>\n",
      "[[-6.98089600e-02  8.54015350e-02 -6.65393919e-02  5.23515716e-02\n",
      "  -1.94934666e-01  3.65341961e-01  3.78372580e-01 -1.82424352e-01\n",
      "  -1.92453265e-01 -2.51381457e-01 -2.89486408e-01  6.00317538e-01\n",
      "  -1.27224207e-01  2.13604644e-01 -1.09502010e-01  7.31219769e-01\n",
      "  -7.53826976e-01 -4.54864144e-01 -4.85928953e-01  7.65500903e-01\n",
      "  -1.97194487e-01  8.91038835e-01  3.30345243e-01 -3.47170830e-02\n",
      "   2.24707723e-01 -2.56163716e-01 -6.18660390e-01 -2.59753376e-01\n",
      "  -2.47774661e-01 -9.01791304e-02  2.12570518e-01  1.98923834e-02\n",
      "   8.64223599e-01  1.74429268e-01 -4.43559498e-01 -5.91391504e-01\n",
      "   7.13081658e-02 -1.26860961e-01  3.62760246e-01  3.81595671e-01\n",
      "   2.52822578e-01  4.50831532e-01 -6.67024255e-02 -4.09743279e-01\n",
      "  -8.63886833e-01  1.03289366e-01  5.56929037e-02 -7.02638179e-03\n",
      "  -4.57333446e-01 -1.59371078e-01 -2.63388336e-01 -4.92135763e-01\n",
      "   4.66339052e-01  3.02299529e-01  2.84311354e-01 -4.22149539e-01\n",
      "  -1.85074866e-01 -1.18439496e-01 -8.29456747e-02  1.38788909e-01\n",
      "  -6.17417753e-01  5.06262064e-01 -4.22994643e-01 -3.01720530e-01\n",
      "   7.40889132e-01 -2.18206421e-02 -5.51719368e-02  4.17423427e-01\n",
      "  -7.51374722e-01  9.66662049e-01 -1.23620942e-01  2.43942477e-02\n",
      "  -5.44345498e-01  2.03314781e-01  1.71246961e-01  6.30418658e-01\n",
      "   7.62838870e-03 -5.77553749e-01 -5.65696657e-01  1.51790857e-01\n",
      "   2.13839114e-02  3.80765051e-01 -4.24079299e-02  1.84686035e-02\n",
      "   1.05913728e-01  9.11011845e-02 -2.16780782e-01 -1.67263538e-01\n",
      "  -3.34832549e-01  1.84831142e-01  5.58998942e-01 -3.32565218e-01\n",
      "  -3.82510722e-01 -8.10633659e-01  1.29385293e-01 -2.94734359e-01\n",
      "  -4.05633569e-01 -6.68912768e-01 -2.76858747e-01  2.78838009e-01\n",
      "   6.03122115e-01  3.19388866e-01 -2.57079601e-01  3.69918585e-01\n",
      "   7.48909473e-01  1.92198128e-01  7.39516616e-01 -3.35763693e-02\n",
      "   2.93584824e-01  1.01626694e-01  2.17019394e-01 -9.21695158e-02\n",
      "  -3.17112148e-01 -3.00213337e-01  4.62725997e-01  2.23826215e-01\n",
      "   9.31920648e-01 -1.22003183e-01  3.30282599e-01 -8.32498074e-04\n",
      "  -2.06879377e-02  4.36114609e-01 -3.90776515e-01  2.86450475e-01\n",
      "   7.29932010e-01 -4.31156218e-01  3.41482610e-01  6.69299424e-01\n",
      "  -4.13531810e-01 -4.70557392e-01 -8.23020995e-01  7.95347154e-01\n",
      "   9.55172256e-02  4.07000870e-01 -7.88537383e-01 -8.02855045e-02\n",
      "   1.17078513e-01 -6.74390197e-01 -2.73518413e-01  1.29126310e-02\n",
      "   3.38183820e-01  7.11941063e-01  1.17725283e-01  1.69081211e-01\n",
      "   3.52880687e-01 -2.08409637e-01 -1.52071372e-01  3.43521893e-01\n",
      "  -5.52651763e-01  1.05961651e-01  4.60620224e-03 -2.09178671e-01\n",
      "  -6.26449585e-01 -5.34286976e-01 -8.77640843e-02 -7.07976818e-01\n",
      "   1.54135570e-01  4.97374773e-01  1.01855505e+00 -2.06008330e-01\n",
      "   3.14051628e-01 -3.97516251e-01 -1.68635398e-01  4.56491768e-01\n",
      "  -1.04570776e-01 -7.76498169e-02  6.03556454e-01 -3.27909231e-01\n",
      "  -1.18100703e-01 -5.18288732e-01 -9.68466699e-02  4.40665126e-01\n",
      "   4.91514713e-01 -3.27414572e-01 -1.83954656e-01 -1.57974184e-01\n",
      "  -1.72771305e-01 -7.68233091e-03  6.01757765e-01  1.43421724e-01\n",
      "   4.24849451e-01 -3.30757082e-01  5.49741805e-01  2.99984105e-02\n",
      "   4.13565218e-01  3.12961638e-04  3.27892959e-01 -3.55832875e-01\n",
      "   7.57537425e-01  4.24327612e-01  4.37935740e-01 -5.50809681e-01\n",
      "  -8.77954960e-02  2.89811194e-01 -1.49724394e-01  1.06310502e-01\n",
      "  -9.31679130e-01 -7.81649768e-01  4.32587504e-01 -2.91012257e-01]]\n",
      "[[0.00000000e+00 8.54015350e-02 0.00000000e+00 5.23515716e-02\n",
      "  0.00000000e+00 3.65341961e-01 3.78372580e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.00317538e-01\n",
      "  0.00000000e+00 2.13604644e-01 0.00000000e+00 7.31219769e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.65500903e-01\n",
      "  0.00000000e+00 8.91038835e-01 3.30345243e-01 0.00000000e+00\n",
      "  2.24707723e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.12570518e-01 1.98923834e-02\n",
      "  8.64223599e-01 1.74429268e-01 0.00000000e+00 0.00000000e+00\n",
      "  7.13081658e-02 0.00000000e+00 3.62760246e-01 3.81595671e-01\n",
      "  2.52822578e-01 4.50831532e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.03289366e-01 5.56929037e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.66339052e-01 3.02299529e-01 2.84311354e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.38788909e-01\n",
      "  0.00000000e+00 5.06262064e-01 0.00000000e+00 0.00000000e+00\n",
      "  7.40889132e-01 0.00000000e+00 0.00000000e+00 4.17423427e-01\n",
      "  0.00000000e+00 9.66662049e-01 0.00000000e+00 2.43942477e-02\n",
      "  0.00000000e+00 2.03314781e-01 1.71246961e-01 6.30418658e-01\n",
      "  7.62838870e-03 0.00000000e+00 0.00000000e+00 1.51790857e-01\n",
      "  2.13839114e-02 3.80765051e-01 0.00000000e+00 1.84686035e-02\n",
      "  1.05913728e-01 9.11011845e-02 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.84831142e-01 5.58998942e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.29385293e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.78838009e-01\n",
      "  6.03122115e-01 3.19388866e-01 0.00000000e+00 3.69918585e-01\n",
      "  7.48909473e-01 1.92198128e-01 7.39516616e-01 0.00000000e+00\n",
      "  2.93584824e-01 1.01626694e-01 2.17019394e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 4.62725997e-01 2.23826215e-01\n",
      "  9.31920648e-01 0.00000000e+00 3.30282599e-01 0.00000000e+00\n",
      "  0.00000000e+00 4.36114609e-01 0.00000000e+00 2.86450475e-01\n",
      "  7.29932010e-01 0.00000000e+00 3.41482610e-01 6.69299424e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.95347154e-01\n",
      "  9.55172256e-02 4.07000870e-01 0.00000000e+00 0.00000000e+00\n",
      "  1.17078513e-01 0.00000000e+00 0.00000000e+00 1.29126310e-02\n",
      "  3.38183820e-01 7.11941063e-01 1.17725283e-01 1.69081211e-01\n",
      "  3.52880687e-01 0.00000000e+00 0.00000000e+00 3.43521893e-01\n",
      "  0.00000000e+00 1.05961651e-01 4.60620224e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.54135570e-01 4.97374773e-01 1.01855505e+00 0.00000000e+00\n",
      "  3.14051628e-01 0.00000000e+00 0.00000000e+00 4.56491768e-01\n",
      "  0.00000000e+00 0.00000000e+00 6.03556454e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.40665126e-01\n",
      "  4.91514713e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.01757765e-01 1.43421724e-01\n",
      "  4.24849451e-01 0.00000000e+00 5.49741805e-01 2.99984105e-02\n",
      "  4.13565218e-01 3.12961638e-04 3.27892959e-01 0.00000000e+00\n",
      "  7.57537425e-01 4.24327612e-01 4.37935740e-01 0.00000000e+00\n",
      "  0.00000000e+00 2.89811194e-01 0.00000000e+00 1.06310502e-01\n",
      "  0.00000000e+00 0.00000000e+00 4.32587504e-01 0.00000000e+00]]\n",
      "[[ -8.125271     0.15900739   7.54518    -17.151155     5.0378437\n",
      "   -7.3349347   -6.15028      8.192185     4.014865    -2.0692587 ]]\n",
      "[[1.24331   9.209574  1.823401  0.        4.330737  2.033646  3.2183008\n",
      "  1.1763954 5.353716  7.299322 ]]\n",
      "[array([9.368581], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[-2]) #last tent layer\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[-5].output)\n",
    "intermediate_output = intermediate_layer_model.predict(flat_gray)\n",
    "print(intermediate_output[:3])\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[-4].output)\n",
    "intermediate_output = intermediate_layer_model.predict(flat_gray)\n",
    "print(intermediate_output[:3])\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[-3].output)\n",
    "intermediate_output = intermediate_layer_model.predict(flat_gray)\n",
    "print(intermediate_output[:3])\n",
    "\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[-2].output)\n",
    "intermediate_output = intermediate_layer_model.predict(flat_gray)\n",
    "print(intermediate_output[:3])\n",
    "\n",
    "print(model.layers[-2].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean confidence 0.84968436\n",
      "accuracy on noisy test set: 0.5706\n",
      "mean confidence 0.8987127\n",
      "accuracy on noisy test set: 0.8886\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "noisy_test = x_test*0.7+np.random.random(np.shape(x_test))*0.3\n",
    "\n",
    "all_confidences = np.max(model.predict(noisy_test),axis=1)\n",
    "print(\"mean confidence\", np.mean(all_confidences))\n",
    "wasit_correct = np.argmax(y_test,axis=1)==np.argmax(model.predict(noisy_test),axis=1)\n",
    "print(\"accuracy on noisy test set:\", np.mean(wasit_correct))\n",
    "\n",
    "\n",
    "order = np.argsort(all_confidences)\n",
    "ordered_confidences = all_confidences[order]\n",
    "ordered_wasit = wasit_correct[order]\n",
    "\n",
    "mean_acc=[]\n",
    "last_idx=0\n",
    "for i in np.arange(0.16,1.01,0.01):\n",
    "    stop_idx = np.searchsorted(ordered_confidences,i)\n",
    "    mean_acc.append(np.mean(ordered_wasit[last_idx:stop_idx]))\n",
    "    last_idx=stop_idx\n",
    "    \n",
    "#print(mean_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0.16,1.01,0.01)+0.01,mean_acc,label=\"tent_in_last layer\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "plt.ylabel(\"accruacy\")\n",
    "plt.xlabel(\"confidence in predicted class\")\n",
    "plt.legend()\n",
    "\n",
    "#####################\n",
    "\n",
    "all_confidences = np.max(baseline.predict(noisy_test),axis=1)\n",
    "print(\"mean confidence\", np.mean(all_confidences))\n",
    "wasit_correct = np.argmax(y_test,axis=1)==np.argmax(baseline.predict(noisy_test),axis=1)\n",
    "print(\"accuracy on noisy test set:\", np.mean(wasit_correct))\n",
    "\n",
    "\n",
    "order = np.argsort(all_confidences)\n",
    "ordered_confidences = all_confidences[order]\n",
    "ordered_wasit = wasit_correct[order]\n",
    "\n",
    "mean_acc=[]\n",
    "last_idx=0\n",
    "for i in np.arange(0.16,1.01,0.01):\n",
    "    stop_idx = np.searchsorted(ordered_confidences,i)\n",
    "    mean_acc.append(np.mean(ordered_wasit[last_idx:stop_idx]))\n",
    "    last_idx=stop_idx\n",
    "    \n",
    "#print(mean_acc)\n",
    "\n",
    "plt.plot(np.arange(0.16,1.01,0.01)+0.01,mean_acc,label=\"relu_baseline\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "plt.ylabel(\"accruacy\")\n",
    "plt.xlabel(\"confidence in predicted class\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "baseline relu does not seem so bad, does it? higher accuracy and decent calibration. Do we even have a problem to solve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is CIFAR10 \n",
    "### This is more interesting than MNIST, because we have similar dataset (cifar100), also, more realistic etc\n",
    "\n",
    "\n",
    "#THE MODELS BELOW WILL RUN ON CPU!\n",
    "### TODO: replace model with alexnet architecture\n",
    "###     1) train baseline without batchnorm\n",
    "###     2) train basleine with BN\n",
    "###     3) train tent in all layers (with BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "255.0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "(x100_train, y100_train), (x100_test, y100_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x100_train = x_train.astype('float32')\n",
    "x100_test = x_test.astype('float32')\n",
    "\n",
    "#x_train /= 255.0\n",
    "#x_test /= 255.0\n",
    "#x100_train /= 255.0\n",
    "#x100_test /= 255.0\n",
    "print(np.shape(x100_train),np.shape(x100_test))\n",
    "print(np.max(x100_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 60, 120, 160 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 0.005\n",
    "    if epoch > 160:\n",
    "        lr *=  0.8**3\n",
    "    elif epoch > 120:\n",
    "        lr *=  0.8**2\n",
    "    elif epoch > 60:\n",
    "        lr *= 0.8\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks\n",
    "baseline_wnn= networks.WideResidualNetwork(width=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be random (/uniform):   [[0.00558265 0.2346106  0.02837992 0.07606066 0.19645056 0.05116265\n",
      "  0.05004261 0.18317674 0.05850454 0.11602917]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for lay in baseline_wnn.layers:\n",
    "    if \"theta_initializer\" in lay.get_config():\n",
    "        print(np.shape(lay.get_weights()))\n",
    "        print(lay.get_weights())\n",
    "\n",
    "\n",
    "output = baseline_wnn.predict(np.expand_dims(np.random.random(x_train.shape[1:]),0))\n",
    "print(\"should be random (/uniform):  \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 16)   64          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 32, 16)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 32, 160)  23200       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 160)  640         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 32, 32, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 32, 32, 160)  230560      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 160)  640         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 160)  2720        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 32, 32, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 32, 32, 160)  0           conv2d_58[0][0]                  \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 160)  230560      add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 32, 160)  640         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 32, 32, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 160)  230560      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 32, 32, 160)  640         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 32, 32, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 32, 32, 160)  0           add_25[0][0]                     \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 32, 32, 160)  230560      add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 32, 32, 160)  640         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 32, 32, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 160)  230560      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 160)  640         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32, 32, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 32, 32, 160)  0           add_26[0][0]                     \n",
      "                                                                 activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 32, 32, 160)  230560      add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 160)  640         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 32, 32, 160)  230560      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 160)  640         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 32, 32, 160)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 32, 32, 160)  0           add_27[0][0]                     \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 160)  0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 320)  1280        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 320)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 16, 16, 320)  921920      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 320)  1280        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 320)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 16, 16, 320)  0           conv2d_67[0][0]                  \n",
      "                                                                 activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 16, 16, 320)  921920      add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 320)  1280        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 320)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 16, 16, 320)  921920      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 320)  1280        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 320)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 16, 16, 320)  0           add_29[0][0]                     \n",
      "                                                                 activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 16, 16, 320)  921920      add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 320)  1280        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 320)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 16, 16, 320)  921920      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 320)  1280        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 320)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 16, 16, 320)  0           add_30[0][0]                     \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 320)  921920      add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 320)  1280        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 320)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 320)  921920      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 320)  1280        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 320)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 16, 16, 320)  0           add_31[0][0]                     \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 8, 8, 320)    0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 640)    2560        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 8, 640)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 640)    3687040     activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 640)    2560        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 8, 640)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 8, 8, 640)    0           conv2d_76[0][0]                  \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 640)    3687040     add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 640)    2560        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 8, 640)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 640)    3687040     activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 640)    2560        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 8, 640)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 8, 8, 640)    0           add_33[0][0]                     \n",
      "                                                                 activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 640)    3687040     add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 640)    2560        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 640)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 640)    3687040     activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 640)    2560        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 8, 8, 640)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 8, 8, 640)    0           add_34[0][0]                     \n",
      "                                                                 activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 640)    3687040     add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 640)    2560        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 8, 8, 640)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 640)    3687040     activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 640)    2560        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 640)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 8, 8, 640)    0           add_35[0][0]                     \n",
      "                                                                 activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 640)          0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           6410        global_average_pooling2d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 36,507,242\n",
      "Trainable params: 36,489,290\n",
      "Non-trainable params: 17,952\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 251s 503ms/step - loss: 3.0241 - acc: 0.3288 - val_loss: 2.1859 - val_acc: 0.3225\n",
      "Epoch 2/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 250s 499ms/step - loss: 1.4699 - acc: 0.5104 - val_loss: 1.4678 - val_acc: 0.5208\n",
      "Epoch 3/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 1.1418 - acc: 0.6209 - val_loss: 1.0250 - val_acc: 0.6407\n",
      "Epoch 4/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 479ms/step - loss: 0.8431 - acc: 0.7046 - val_loss: 1.1425 - val_acc: 0.6255\n",
      "Epoch 5/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 0.6885 - acc: 0.7571 - val_loss: 0.7862 - val_acc: 0.7408\n",
      "Epoch 6/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 249s 497ms/step - loss: 0.5937 - acc: 0.7916 - val_loss: 0.6586 - val_acc: 0.7748\n",
      "Epoch 7/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 246s 491ms/step - loss: 0.5268 - acc: 0.8176 - val_loss: 0.6408 - val_acc: 0.7835\n",
      "Epoch 8/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 246s 491ms/step - loss: 0.4721 - acc: 0.8355 - val_loss: 0.5771 - val_acc: 0.8086\n",
      "Epoch 9/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 245s 490ms/step - loss: 0.4259 - acc: 0.8534 - val_loss: 0.5291 - val_acc: 0.8223\n",
      "Epoch 10/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 0.3918 - acc: 0.8637 - val_loss: 0.5730 - val_acc: 0.8062\n",
      "Epoch 11/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 0.3574 - acc: 0.8745 - val_loss: 0.5403 - val_acc: 0.8232\n",
      "Epoch 12/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 0.3310 - acc: 0.8836 - val_loss: 0.5785 - val_acc: 0.8033\n",
      "Epoch 13/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 242s 484ms/step - loss: 0.3096 - acc: 0.8916 - val_loss: 0.5598 - val_acc: 0.8262\n",
      "Epoch 14/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.2835 - acc: 0.8999 - val_loss: 0.5215 - val_acc: 0.8330\n",
      "Epoch 15/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.2603 - acc: 0.9102 - val_loss: 0.6044 - val_acc: 0.8155\n",
      "Epoch 16/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.2471 - acc: 0.9142 - val_loss: 0.6179 - val_acc: 0.8120\n",
      "Epoch 17/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.2253 - acc: 0.9217 - val_loss: 0.4372 - val_acc: 0.8554\n",
      "Epoch 18/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.2092 - acc: 0.9268 - val_loss: 0.4739 - val_acc: 0.8484\n",
      "Epoch 19/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1969 - acc: 0.9318 - val_loss: 0.5263 - val_acc: 0.8333\n",
      "Epoch 20/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1787 - acc: 0.9385 - val_loss: 0.4436 - val_acc: 0.8625\n",
      "Epoch 21/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1697 - acc: 0.9409 - val_loss: 0.4739 - val_acc: 0.8575\n",
      "Epoch 22/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1555 - acc: 0.9467 - val_loss: 0.5773 - val_acc: 0.8293\n",
      "Epoch 23/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1456 - acc: 0.9496 - val_loss: 0.4876 - val_acc: 0.8525\n",
      "Epoch 24/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1381 - acc: 0.9529 - val_loss: 0.4495 - val_acc: 0.8626\n",
      "Epoch 25/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1289 - acc: 0.9552 - val_loss: 0.5160 - val_acc: 0.8533\n",
      "Epoch 26/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1145 - acc: 0.9614 - val_loss: 0.4377 - val_acc: 0.8744\n",
      "Epoch 27/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.1113 - acc: 0.9611 - val_loss: 0.4818 - val_acc: 0.8643\n",
      "Epoch 28/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0989 - acc: 0.9670 - val_loss: 0.5514 - val_acc: 0.8469\n",
      "Epoch 29/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0964 - acc: 0.9669 - val_loss: 0.4931 - val_acc: 0.8592\n",
      "Epoch 30/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0907 - acc: 0.9688 - val_loss: 0.5768 - val_acc: 0.8517\n",
      "Epoch 31/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0813 - acc: 0.9729 - val_loss: 0.4833 - val_acc: 0.8682\n",
      "Epoch 32/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0781 - acc: 0.9746 - val_loss: 0.4187 - val_acc: 0.8811\n",
      "Epoch 33/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0723 - acc: 0.9760 - val_loss: 0.4079 - val_acc: 0.8820\n",
      "Epoch 34/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0679 - acc: 0.9776 - val_loss: 0.4436 - val_acc: 0.8749\n",
      "Epoch 35/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0658 - acc: 0.9783 - val_loss: 0.5849 - val_acc: 0.8495\n",
      "Epoch 36/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0609 - acc: 0.9795 - val_loss: 0.4207 - val_acc: 0.8828\n",
      "Epoch 37/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0592 - acc: 0.9804 - val_loss: 0.4443 - val_acc: 0.8782\n",
      "Epoch 38/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0546 - acc: 0.9824 - val_loss: 0.4850 - val_acc: 0.8790\n",
      "Epoch 39/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0512 - acc: 0.9840 - val_loss: 0.4823 - val_acc: 0.8765\n",
      "Epoch 40/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0464 - acc: 0.9850 - val_loss: 0.4492 - val_acc: 0.8835\n",
      "Epoch 41/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0450 - acc: 0.9859 - val_loss: 0.4567 - val_acc: 0.8811\n",
      "Epoch 42/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0443 - acc: 0.9861 - val_loss: 0.4916 - val_acc: 0.8745\n",
      "Epoch 43/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0442 - acc: 0.9860 - val_loss: 0.4592 - val_acc: 0.8796\n",
      "Epoch 44/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0406 - acc: 0.9867 - val_loss: 0.5110 - val_acc: 0.8676\n",
      "Epoch 45/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0355 - acc: 0.9892 - val_loss: 0.4571 - val_acc: 0.8786\n",
      "Epoch 46/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0339 - acc: 0.9897 - val_loss: 0.5138 - val_acc: 0.8757\n",
      "Epoch 47/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0346 - acc: 0.9892 - val_loss: 0.4477 - val_acc: 0.8836\n",
      "Epoch 48/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0322 - acc: 0.9900 - val_loss: 0.4519 - val_acc: 0.8871\n",
      "Epoch 49/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0320 - acc: 0.9900 - val_loss: 0.4739 - val_acc: 0.8821\n",
      "Epoch 50/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0304 - acc: 0.9908 - val_loss: 0.5425 - val_acc: 0.8723\n",
      "Epoch 51/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0275 - acc: 0.9918 - val_loss: 0.4649 - val_acc: 0.8838\n",
      "Epoch 52/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0262 - acc: 0.9927 - val_loss: 0.4731 - val_acc: 0.8885\n",
      "Epoch 53/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0256 - acc: 0.9925 - val_loss: 0.5053 - val_acc: 0.8806\n",
      "Epoch 54/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0240 - acc: 0.9929 - val_loss: 0.4763 - val_acc: 0.8872\n",
      "Epoch 55/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0228 - acc: 0.9933 - val_loss: 0.4644 - val_acc: 0.8846\n",
      "Epoch 56/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0234 - acc: 0.9934 - val_loss: 0.4666 - val_acc: 0.8884\n",
      "Epoch 57/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0233 - acc: 0.9931 - val_loss: 0.4671 - val_acc: 0.8836\n",
      "Epoch 58/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0224 - acc: 0.9935 - val_loss: 0.4533 - val_acc: 0.8875\n",
      "Epoch 59/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0207 - acc: 0.9945 - val_loss: 0.4887 - val_acc: 0.8886\n",
      "Epoch 60/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0221 - acc: 0.9937 - val_loss: 0.4915 - val_acc: 0.8822\n",
      "Epoch 61/200\n",
      "Learning rate:  0.005\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.4837 - val_acc: 0.8918\n",
      "Epoch 62/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0161 - acc: 0.9958 - val_loss: 0.4492 - val_acc: 0.8909\n",
      "Epoch 63/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0166 - acc: 0.9955 - val_loss: 0.4425 - val_acc: 0.8920\n",
      "Epoch 64/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.4491 - val_acc: 0.8931\n",
      "Epoch 65/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0142 - acc: 0.9964 - val_loss: 0.4440 - val_acc: 0.8928\n",
      "Epoch 66/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0161 - acc: 0.9954 - val_loss: 0.4789 - val_acc: 0.8871\n",
      "Epoch 67/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0144 - acc: 0.9962 - val_loss: 0.4952 - val_acc: 0.8855\n",
      "Epoch 68/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.4767 - val_acc: 0.8893\n",
      "Epoch 69/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0142 - acc: 0.9965 - val_loss: 0.4495 - val_acc: 0.8940\n",
      "Epoch 70/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0137 - acc: 0.9965 - val_loss: 0.4549 - val_acc: 0.8927\n",
      "Epoch 71/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0132 - acc: 0.9971 - val_loss: 0.4668 - val_acc: 0.8900\n",
      "Epoch 72/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.4491 - val_acc: 0.8928\n",
      "Epoch 73/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.4517 - val_acc: 0.8945\n",
      "Epoch 74/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0126 - acc: 0.9972 - val_loss: 0.4608 - val_acc: 0.8945\n",
      "Epoch 75/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.4524 - val_acc: 0.8929\n",
      "Epoch 76/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.4578 - val_acc: 0.8909\n",
      "Epoch 77/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0114 - acc: 0.9974 - val_loss: 0.4635 - val_acc: 0.8934\n",
      "Epoch 78/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0112 - acc: 0.9972 - val_loss: 0.4672 - val_acc: 0.8938\n",
      "Epoch 79/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.4621 - val_acc: 0.8932\n",
      "Epoch 80/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0109 - acc: 0.9973 - val_loss: 0.4560 - val_acc: 0.8939\n",
      "Epoch 81/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0109 - acc: 0.9973 - val_loss: 0.4620 - val_acc: 0.8949\n",
      "Epoch 82/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0113 - acc: 0.9971 - val_loss: 0.4716 - val_acc: 0.8931\n",
      "Epoch 83/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0103 - acc: 0.9978 - val_loss: 0.4526 - val_acc: 0.8945\n",
      "Epoch 84/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0108 - acc: 0.9973 - val_loss: 0.4690 - val_acc: 0.8911\n",
      "Epoch 85/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0102 - acc: 0.9974 - val_loss: 0.4652 - val_acc: 0.8963\n",
      "Epoch 86/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.4548 - val_acc: 0.8947\n",
      "Epoch 87/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0101 - acc: 0.9975 - val_loss: 0.4584 - val_acc: 0.8940\n",
      "Epoch 88/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0090 - acc: 0.9980 - val_loss: 0.4700 - val_acc: 0.8930\n",
      "Epoch 89/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.4782 - val_acc: 0.8899\n",
      "Epoch 90/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0101 - acc: 0.9975 - val_loss: 0.4604 - val_acc: 0.8938\n",
      "Epoch 91/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0102 - acc: 0.9975 - val_loss: 0.4676 - val_acc: 0.8922\n",
      "Epoch 92/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0089 - acc: 0.9977 - val_loss: 0.4622 - val_acc: 0.8937\n",
      "Epoch 93/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0097 - acc: 0.9977 - val_loss: 0.4623 - val_acc: 0.8956\n",
      "Epoch 94/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0093 - acc: 0.9980 - val_loss: 0.4793 - val_acc: 0.8939\n",
      "Epoch 95/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0095 - acc: 0.9980 - val_loss: 0.4573 - val_acc: 0.8945\n",
      "Epoch 96/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0091 - acc: 0.9977 - val_loss: 0.4709 - val_acc: 0.8933\n",
      "Epoch 97/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0092 - acc: 0.9979 - val_loss: 0.4709 - val_acc: 0.8921\n",
      "Epoch 98/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.4651 - val_acc: 0.8960\n",
      "Epoch 99/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.4886 - val_acc: 0.8894\n",
      "Epoch 100/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0082 - acc: 0.9983 - val_loss: 0.4651 - val_acc: 0.8955\n",
      "Epoch 101/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0075 - acc: 0.9984 - val_loss: 0.4744 - val_acc: 0.8907\n",
      "Epoch 102/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0086 - acc: 0.9982 - val_loss: 0.4713 - val_acc: 0.8948\n",
      "Epoch 103/200\n",
      "Learning rate:  0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.4868 - val_acc: 0.8912\n",
      "Epoch 104/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0080 - acc: 0.9983 - val_loss: 0.4614 - val_acc: 0.8954\n",
      "Epoch 105/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0081 - acc: 0.9984 - val_loss: 0.4680 - val_acc: 0.8937\n",
      "Epoch 106/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0077 - acc: 0.9985 - val_loss: 0.4688 - val_acc: 0.8958\n",
      "Epoch 107/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0082 - acc: 0.9981 - val_loss: 0.4608 - val_acc: 0.8960\n",
      "Epoch 108/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0073 - acc: 0.9985 - val_loss: 0.5056 - val_acc: 0.8903\n",
      "Epoch 109/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0077 - acc: 0.9983 - val_loss: 0.4778 - val_acc: 0.8974\n",
      "Epoch 110/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0075 - acc: 0.9983 - val_loss: 0.4648 - val_acc: 0.8969\n",
      "Epoch 111/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0073 - acc: 0.9983 - val_loss: 0.4751 - val_acc: 0.8958\n",
      "Epoch 112/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0073 - acc: 0.9983 - val_loss: 0.4698 - val_acc: 0.8963\n",
      "Epoch 113/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0077 - acc: 0.9984 - val_loss: 0.4870 - val_acc: 0.8940\n",
      "Epoch 114/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.4705 - val_acc: 0.8932\n",
      "Epoch 115/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.4714 - val_acc: 0.8950\n",
      "Epoch 116/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.4678 - val_acc: 0.8943\n",
      "Epoch 117/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0066 - acc: 0.9987 - val_loss: 0.4846 - val_acc: 0.8954\n",
      "Epoch 118/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.4613 - val_acc: 0.8969\n",
      "Epoch 119/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.4707 - val_acc: 0.8967\n",
      "Epoch 120/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0066 - acc: 0.9987 - val_loss: 0.4702 - val_acc: 0.8955\n",
      "Epoch 121/200\n",
      "Learning rate:  0.004\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.4673 - val_acc: 0.8967\n",
      "Epoch 122/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0065 - acc: 0.9986 - val_loss: 0.4703 - val_acc: 0.8953\n",
      "Epoch 123/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0061 - acc: 0.9990 - val_loss: 0.4654 - val_acc: 0.8978\n",
      "Epoch 124/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0067 - acc: 0.9984 - val_loss: 0.4735 - val_acc: 0.8958\n",
      "Epoch 125/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.4684 - val_acc: 0.8970\n",
      "Epoch 126/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.4785 - val_acc: 0.8943\n",
      "Epoch 127/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0063 - acc: 0.9987 - val_loss: 0.4727 - val_acc: 0.8962\n",
      "Epoch 128/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.4690 - val_acc: 0.8951\n",
      "Epoch 129/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0060 - acc: 0.9989 - val_loss: 0.4636 - val_acc: 0.8974\n",
      "Epoch 130/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0070 - acc: 0.9985 - val_loss: 0.4681 - val_acc: 0.8962\n",
      "Epoch 131/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.4661 - val_acc: 0.8970\n",
      "Epoch 132/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.4655 - val_acc: 0.8971\n",
      "Epoch 133/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.4647 - val_acc: 0.8981\n",
      "Epoch 134/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.4730 - val_acc: 0.8978\n",
      "Epoch 135/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.4758 - val_acc: 0.8938\n",
      "Epoch 136/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.4701 - val_acc: 0.8967\n",
      "Epoch 137/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.4680 - val_acc: 0.8982\n",
      "Epoch 138/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.4724 - val_acc: 0.8975\n",
      "Epoch 139/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0052 - acc: 0.9993 - val_loss: 0.4700 - val_acc: 0.8982\n",
      "Epoch 140/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.4708 - val_acc: 0.8989\n",
      "Epoch 141/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.4699 - val_acc: 0.8973\n",
      "Epoch 142/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.4783 - val_acc: 0.8955\n",
      "Epoch 143/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.4697 - val_acc: 0.8986\n",
      "Epoch 144/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.4715 - val_acc: 0.8969\n",
      "Epoch 145/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.4670 - val_acc: 0.8985\n",
      "Epoch 146/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.4708 - val_acc: 0.8979\n",
      "Epoch 147/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0052 - acc: 0.9990 - val_loss: 0.4661 - val_acc: 0.8980\n",
      "Epoch 148/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0054 - acc: 0.9990 - val_loss: 0.4773 - val_acc: 0.8964\n",
      "Epoch 149/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.4688 - val_acc: 0.8986\n",
      "Epoch 150/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.4728 - val_acc: 0.8962\n",
      "Epoch 151/200\n",
      "Learning rate:  0.0032000000000000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.4705 - val_acc: 0.8982\n",
      "Epoch 152/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.4771 - val_acc: 0.8974\n",
      "Epoch 153/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.4705 - val_acc: 0.8985\n",
      "Epoch 154/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0054 - acc: 0.9990 - val_loss: 0.4745 - val_acc: 0.8989\n",
      "Epoch 155/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0054 - acc: 0.9990 - val_loss: 0.4786 - val_acc: 0.8959\n",
      "Epoch 156/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.4746 - val_acc: 0.8967\n",
      "Epoch 157/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.4721 - val_acc: 0.8979\n",
      "Epoch 158/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0050 - acc: 0.9991 - val_loss: 0.4798 - val_acc: 0.8958\n",
      "Epoch 159/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.4737 - val_acc: 0.8980\n",
      "Epoch 160/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0054 - acc: 0.9989 - val_loss: 0.4724 - val_acc: 0.8975\n",
      "Epoch 161/200\n",
      "Learning rate:  0.0032000000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.4729 - val_acc: 0.8977\n",
      "Epoch 162/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.4759 - val_acc: 0.8964\n",
      "Epoch 163/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.4732 - val_acc: 0.8971\n",
      "Epoch 164/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0047 - acc: 0.9993 - val_loss: 0.4737 - val_acc: 0.8966\n",
      "Epoch 165/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.4731 - val_acc: 0.8984\n",
      "Epoch 166/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.4691 - val_acc: 0.8981\n",
      "Epoch 167/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0045 - acc: 0.9991 - val_loss: 0.4715 - val_acc: 0.8985\n",
      "Epoch 168/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0048 - acc: 0.9989 - val_loss: 0.4768 - val_acc: 0.8975\n",
      "Epoch 169/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.4727 - val_acc: 0.8973\n",
      "Epoch 170/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.4714 - val_acc: 0.8974\n",
      "Epoch 171/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.4749 - val_acc: 0.8976\n",
      "Epoch 172/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0047 - acc: 0.9992 - val_loss: 0.4768 - val_acc: 0.8969\n",
      "Epoch 173/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0050 - acc: 0.9991 - val_loss: 0.4784 - val_acc: 0.8985\n",
      "Epoch 174/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0048 - acc: 0.9992 - val_loss: 0.4724 - val_acc: 0.8985\n",
      "Epoch 175/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.4688 - val_acc: 0.8967\n",
      "Epoch 176/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.4700 - val_acc: 0.8967\n",
      "Epoch 177/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.4694 - val_acc: 0.8990\n",
      "Epoch 178/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.4736 - val_acc: 0.8969\n",
      "Epoch 179/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0041 - acc: 0.9995 - val_loss: 0.4734 - val_acc: 0.8979\n",
      "Epoch 180/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0042 - acc: 0.9994 - val_loss: 0.4717 - val_acc: 0.8972\n",
      "Epoch 181/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.4746 - val_acc: 0.8971\n",
      "Epoch 182/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0044 - acc: 0.9993 - val_loss: 0.4748 - val_acc: 0.8972\n",
      "Epoch 183/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0044 - acc: 0.9993 - val_loss: 0.4722 - val_acc: 0.8982\n",
      "Epoch 184/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.4725 - val_acc: 0.8975\n",
      "Epoch 185/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0041 - acc: 0.9993 - val_loss: 0.4745 - val_acc: 0.8983\n",
      "Epoch 186/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.4718 - val_acc: 0.8987\n",
      "Epoch 187/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.4720 - val_acc: 0.8979\n",
      "Epoch 188/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.4745 - val_acc: 0.8978\n",
      "Epoch 189/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0041 - acc: 0.9993 - val_loss: 0.4701 - val_acc: 0.8997\n",
      "Epoch 190/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 481ms/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.4687 - val_acc: 0.9000\n",
      "Epoch 191/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0045 - acc: 0.9992 - val_loss: 0.4734 - val_acc: 0.8970\n",
      "Epoch 192/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0042 - acc: 0.9992 - val_loss: 0.4674 - val_acc: 0.8986\n",
      "Epoch 193/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.4717 - val_acc: 0.8979\n",
      "Epoch 194/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0045 - acc: 0.9992 - val_loss: 0.4709 - val_acc: 0.8973\n",
      "Epoch 195/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0045 - acc: 0.9990 - val_loss: 0.4734 - val_acc: 0.8991\n",
      "Epoch 196/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.4732 - val_acc: 0.8974\n",
      "Epoch 197/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.4717 - val_acc: 0.8979\n",
      "Epoch 198/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.4778 - val_acc: 0.8987\n",
      "Epoch 199/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.4735 - val_acc: 0.8982\n",
      "Epoch 200/200\n",
      "Learning rate:  0.0025600000000000006\n",
      "500/500 [==============================] - 240s 480ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.4781 - val_acc: 0.8990\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras.layers import activations\n",
    "from keras.layers import initializers\n",
    "from keras.layers import regularizers\n",
    "from keras.layers import constraints\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print(baseline_wnn.summary())\n",
    "\n",
    "baseline_wnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.1,decay=0.0005, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "data_augmentation = True\n",
    "\n",
    "if not data_augmentation:\n",
    "    baseline_wnn.fit(x_train, to_categorical(y_train), batch_size=100, epochs=200, validation_data=(x_test, to_categorical(y_test)), shuffle=True,callbacks=callbacks)\n",
    "    \n",
    "\n",
    "else:\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    baseline_wnn.fit_generator(datagen.flow(x_train, to_categorical(y_train),\n",
    "                                     batch_size=100),\n",
    "                        epochs=200, steps_per_epoch=500,\n",
    "                        validation_data=(x_test, to_categorical(y_test)),\n",
    "                        workers=4, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lay in baseline_wnn.layers:\n",
    "    if \"theta_initializer\" in lay.get_config():\n",
    "        print(np.shape(lay.get_weights()))\n",
    "        print(lay.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_wnn mean confidence on train data  0.99780244\n",
      "baseline_wnn mean confidence on test data  0.9662047\n",
      "\n",
      "\n",
      "\n",
      "baseline_wnn mean confidence on random matrix  0.88714707\n",
      "baseline_wnn mean confidence on flat_gray img  0.6624398\n",
      "baseline_wnn mean confidence on avg_img  0.78461957\n"
     ]
    }
   ],
   "source": [
    "print(\"baseline_wnn mean confidence on train data \",np.mean(np.max(baseline_wnn.predict(x_train),axis=1)))\n",
    "print(\"baseline_wnn mean confidence on test data \",np.mean(np.max(baseline_wnn.predict(x_test),axis=1)))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "ran = np.random.random(size=(100,32,32,3))*255\n",
    "\n",
    "print(\"baseline_wnn mean confidence on random matrix \",np.mean(np.max(baseline_wnn.predict(ran),axis=1)))\n",
    "\n",
    "flat_gray=np.ones((100,32,32,3))*128\n",
    "print(\"baseline_wnn mean confidence on flat_gray img \",np.mean(np.max(baseline_wnn.predict(flat_gray),axis=1)))\n",
    "\n",
    "avg_img = np.mean(x_test, axis=0,keepdims=True)\n",
    "#print(np.shape(avg_img))\n",
    "print(\"baseline_wnn mean confidence on avg_img \",np.mean(np.max(baseline_wnn.predict(avg_img),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car : 0\n",
      "deer : 0\n",
      "dog : 0\n",
      "plane : 1\n",
      "cat : 1\n",
      "frog : 19\n",
      "bird : 79\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(np.argmax(baseline_wnn.predict(ran),axis=1)) \n",
    "names = [\"plane\",\"car\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\", \"ship\",\"truck\"]\n",
    "for i in np.argsort(counts):\n",
    "    print(names[i], \":\", counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent mean confidence on cifar100  0.9234621\n"
     ]
    }
   ],
   "source": [
    "cifar100preds = baseline_wnn.predict(x100_test)\n",
    "print(\"tent mean confidence on cifar100 \",np.mean(np.max(cifar100preds,axis=1)))\n",
    "#print(cifar100preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_wnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c33b53d34eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline_28_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_wnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#baseline results 28-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m  \u001b[0macc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.9483\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.4382\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.8689\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_wnn' is not defined"
     ]
    }
   ],
   "source": [
    "baseline_28_1 = baseline_wnn\n",
    "#baseline results 28-1\n",
    "\n",
    "\"\"\"\n",
    " acc: 0.9483 - val_loss: 0.4382 - val_acc: 0.8689\n",
    "\n",
    "tent mean confidence on cifar100  0.9234621\n",
    "\n",
    "baseline_wnn mean confidence on train data  0.944834\n",
    "baseline_wnn mean confidence on test data  0.9234621\n",
    "\n",
    "\n",
    "\n",
    "baseline_wnn mean confidence on random matrix  0.913423\n",
    "baseline_wnn mean confidence on flat_gray img  0.9578364\n",
    "baseline_wnn mean confidence on avg_img  0.77367663\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254.99985038451163\n",
      "mean confidence 0.878901\n",
      "accuracy on noisy test set: 0.0747\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "noisy_test = x_test*0.9+np.random.random(np.shape(x_test))*0.1*255\n",
    "\n",
    "print(np.max(noisy_test))\n",
    "\n",
    "all_confidences = np.max(baseline_wnn.predict(noisy_test),axis=1)\n",
    "print(\"mean confidence\", np.mean(all_confidences))\n",
    "wasit_correct = np.argmax(y_test,axis=1)==np.argmax(baseline_wnn.predict(noisy_test),axis=1)\n",
    "print(\"accuracy on noisy test set:\", np.mean(wasit_correct))\n",
    "\n",
    "\n",
    "order = np.argsort(all_confidences)\n",
    "ordered_confidences = all_confidences[order]\n",
    "ordered_wasit = wasit_correct[order]\n",
    "\n",
    "mean_acc=[]\n",
    "last_idx=0\n",
    "for i in np.arange(0.16,1.01,0.01):\n",
    "    stop_idx = np.searchsorted(ordered_confidences,i)\n",
    "    mean_acc.append(np.mean(ordered_wasit[last_idx:stop_idx]))\n",
    "    last_idx=stop_idx\n",
    "    \n",
    "#print(mean_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0.16,1.01,0.01)+0.01,mean_acc,label=\"baseline WRN on noisy data\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "plt.ylabel(\"accruacy\")\n",
    "plt.xlabel(\"confidence in predicted class\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRN-28-1 With TENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "255.0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "(x100_train, y100_train), (x100_test, y100_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x100_train = x_train.astype('float32')\n",
    "x100_test = x_test.astype('float32')\n",
    "\n",
    "#x_train /= 255.0\n",
    "#x_test /= 255.0\n",
    "#x100_train /= 255.0\n",
    "#x100_test /= 255.0\n",
    "print(np.shape(x100_train),np.shape(x100_test))\n",
    "print(np.max(x100_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "(1, 1, 1, 1)\n",
      "[array([[[1.]]], dtype=float32)]\n",
      "should be random (/uniform):   [[4.0652804e-04 3.1105904e-02 3.1821823e-03 1.7683471e-02 4.3475339e-08\n",
      "  1.0448222e-02 1.8531231e-06 7.8854231e-05 7.8502637e-01 1.5206657e-01]]\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "tent_wnn= networks.WideResidualNetworkWithTent(width=10,tent_regularizer=0.004)\n",
    "\n",
    "for lay in tent_wnn.layers:\n",
    "    if \"theta_initializer\" in lay.get_config():\n",
    "        print(np.shape(lay.get_weights()))\n",
    "        print(lay.get_weights())\n",
    "\n",
    "\n",
    "output = tent_wnn.predict(np.expand_dims(np.random.random(x_train.shape[1:]),0))\n",
    "print(\"should be random (/uniform):  \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 32, 32, 16)   448         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 32, 32, 16)   64          conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_126 (Tent)                 (None, 32, 32, 16)   1           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 32, 32, 160)  23200       tent_126[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 32, 32, 160)  640         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_127 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 32, 32, 160)  230560      tent_127[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 32, 32, 160)  640         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 32, 32, 160)  2720        tent_126[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tent_128 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 32, 32, 160)  0           conv2d_137[0][0]                 \n",
      "                                                                 tent_128[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 32, 32, 160)  230560      add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 32, 32, 160)  640         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_129 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 32, 32, 160)  230560      tent_129[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 32, 32, 160)  640         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_130 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 32, 32, 160)  0           add_61[0][0]                     \n",
      "                                                                 tent_130[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 32, 32, 160)  230560      add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 32, 32, 160)  640         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_131 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 32, 32, 160)  230560      tent_131[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 32, 32, 160)  640         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_132 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 32, 32, 160)  0           add_62[0][0]                     \n",
      "                                                                 tent_132[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 32, 32, 160)  230560      add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 32, 32, 160)  640         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_133 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 32, 32, 160)  230560      tent_133[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 32, 32, 160)  640         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_134 (Tent)                 (None, 32, 32, 160)  1           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 32, 32, 160)  0           add_63[0][0]                     \n",
      "                                                                 tent_134[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 16, 16, 160)  0           add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 16, 320)  461120      max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 320)  1280        conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_135 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 16, 320)  921920      tent_135[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 320)  1280        conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 16, 320)  51520       max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tent_136 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 16, 16, 320)  0           conv2d_146[0][0]                 \n",
      "                                                                 tent_136[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 16, 320)  921920      add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 16, 16, 320)  1280        conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_137 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 320)  921920      tent_137[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 16, 16, 320)  1280        conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_138 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 16, 16, 320)  0           add_65[0][0]                     \n",
      "                                                                 tent_138[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 320)  921920      add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 16, 16, 320)  1280        conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_139 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 320)  921920      tent_139[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 16, 16, 320)  1280        conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_140 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 16, 16, 320)  0           add_66[0][0]                     \n",
      "                                                                 tent_140[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 16, 16, 320)  921920      add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 320)  1280        conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_141 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 16, 16, 320)  921920      tent_141[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 320)  1280        conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_142 (Tent)                 (None, 16, 16, 320)  1           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 16, 16, 320)  0           add_67[0][0]                     \n",
      "                                                                 tent_142[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 8, 8, 320)    0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 8, 8, 640)    1843840     max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 8, 8, 640)    2560        conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_143 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 8, 8, 640)    3687040     tent_143[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 8, 8, 640)    2560        conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 8, 8, 640)    205440      max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tent_144 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 8, 8, 640)    0           conv2d_155[0][0]                 \n",
      "                                                                 tent_144[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 8, 8, 640)    3687040     add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 8, 8, 640)    2560        conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_145 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 8, 8, 640)    3687040     tent_145[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 8, 8, 640)    2560        conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_146 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 8, 8, 640)    0           add_69[0][0]                     \n",
      "                                                                 tent_146[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 8, 8, 640)    3687040     add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 8, 8, 640)    2560        conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_147 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 8, 8, 640)    3687040     tent_147[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 8, 8, 640)    2560        conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_148 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 8, 8, 640)    0           add_70[0][0]                     \n",
      "                                                                 tent_148[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 8, 8, 640)    3687040     add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 8, 8, 640)    2560        conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_149 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 8, 8, 640)    3687040     tent_149[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 8, 8, 640)    2560        conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tent_150 (Tent)                 (None, 8, 8, 640)    1           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 8, 8, 640)    0           add_71[0][0]                     \n",
      "                                                                 tent_150[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 640)          0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           6410        global_average_pooling2d_6[0][0] \n",
      "==================================================================================================\n",
      "Total params: 36,507,267\n",
      "Trainable params: 36,489,315\n",
      "Non-trainable params: 17,952\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 324s 648ms/step - loss: 5.3440 - acc: 0.2428 - val_loss: 2.6619 - val_acc: 0.1253\n",
      "Epoch 2/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 614ms/step - loss: 1.6914 - acc: 0.4174 - val_loss: 2.6407 - val_acc: 0.1047\n",
      "Epoch 3/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 614ms/step - loss: 1.5469 - acc: 0.4722 - val_loss: 4.2700 - val_acc: 0.1201\n",
      "Epoch 4/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 614ms/step - loss: 1.4587 - acc: 0.5045 - val_loss: 2.7104 - val_acc: 0.1573\n",
      "Epoch 5/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 1.3850 - acc: 0.5296 - val_loss: 2.9757 - val_acc: 0.1098\n",
      "Epoch 6/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 1.3254 - acc: 0.5514 - val_loss: 2.3164 - val_acc: 0.2135\n",
      "Epoch 7/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 614ms/step - loss: 1.2610 - acc: 0.5774 - val_loss: 2.0578 - val_acc: 0.2514\n",
      "Epoch 8/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 1.2078 - acc: 0.5935 - val_loss: 2.2619 - val_acc: 0.2726\n",
      "Epoch 9/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 615ms/step - loss: 1.1608 - acc: 0.6119 - val_loss: 2.3766 - val_acc: 0.1834\n",
      "Epoch 10/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 306s 613ms/step - loss: 1.1032 - acc: 0.6331 - val_loss: 1.8429 - val_acc: 0.3506\n",
      "Epoch 11/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 614ms/step - loss: 1.0361 - acc: 0.6545 - val_loss: 2.0019 - val_acc: 0.3097\n",
      "Epoch 12/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 298s 596ms/step - loss: 0.9812 - acc: 0.6741 - val_loss: 1.4777 - val_acc: 0.5016\n",
      "Epoch 13/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.9233 - acc: 0.6966 - val_loss: 1.8060 - val_acc: 0.3881\n",
      "Epoch 14/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.8828 - acc: 0.7089 - val_loss: 1.6390 - val_acc: 0.4347\n",
      "Epoch 15/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.8560 - acc: 0.7193 - val_loss: 2.2846 - val_acc: 0.2943\n",
      "Epoch 16/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.8130 - acc: 0.7339 - val_loss: 1.2849 - val_acc: 0.5577\n",
      "Epoch 17/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.7686 - acc: 0.7491 - val_loss: 1.1441 - val_acc: 0.6173\n",
      "Epoch 18/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.6943 - acc: 0.7725 - val_loss: 1.2382 - val_acc: 0.5776\n",
      "Epoch 19/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.6474 - acc: 0.7902 - val_loss: 1.1247 - val_acc: 0.6276\n",
      "Epoch 20/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.6111 - acc: 0.8018 - val_loss: 1.0307 - val_acc: 0.6518\n",
      "Epoch 21/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.5650 - acc: 0.8189 - val_loss: 0.9807 - val_acc: 0.6778\n",
      "Epoch 22/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.5376 - acc: 0.8275 - val_loss: 1.1035 - val_acc: 0.6504\n",
      "Epoch 23/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.5535 - acc: 0.8233 - val_loss: 1.5477 - val_acc: 0.5223\n",
      "Epoch 24/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.5186 - acc: 0.8334 - val_loss: 1.0025 - val_acc: 0.6679\n",
      "Epoch 25/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.5104 - acc: 0.8355 - val_loss: 1.0562 - val_acc: 0.6635\n",
      "Epoch 26/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4756 - acc: 0.8483 - val_loss: 1.3664 - val_acc: 0.5813\n",
      "Epoch 27/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4832 - acc: 0.8450 - val_loss: 1.5501 - val_acc: 0.5088\n",
      "Epoch 28/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4756 - acc: 0.8469 - val_loss: 1.2045 - val_acc: 0.6288\n",
      "Epoch 29/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4780 - acc: 0.8449 - val_loss: 1.4343 - val_acc: 0.5666\n",
      "Epoch 30/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4640 - acc: 0.8526 - val_loss: 1.1313 - val_acc: 0.6554\n",
      "Epoch 31/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4334 - acc: 0.8615 - val_loss: 1.1109 - val_acc: 0.6719\n",
      "Epoch 32/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4466 - acc: 0.8573 - val_loss: 1.5623 - val_acc: 0.5584\n",
      "Epoch 33/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.4108 - acc: 0.8699 - val_loss: 1.1332 - val_acc: 0.6505\n",
      "Epoch 34/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3700 - acc: 0.8840 - val_loss: 1.0436 - val_acc: 0.6939\n",
      "Epoch 35/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3682 - acc: 0.8860 - val_loss: 1.0667 - val_acc: 0.6880\n",
      "Epoch 36/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3886 - acc: 0.8769 - val_loss: 1.7597 - val_acc: 0.5533\n",
      "Epoch 37/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3832 - acc: 0.8791 - val_loss: 1.4215 - val_acc: 0.5942\n",
      "Epoch 38/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3706 - acc: 0.8839 - val_loss: 1.2785 - val_acc: 0.6350\n",
      "Epoch 39/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3827 - acc: 0.8777 - val_loss: 1.1346 - val_acc: 0.6563\n",
      "Epoch 40/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3434 - acc: 0.8930 - val_loss: 0.8646 - val_acc: 0.7412\n",
      "Epoch 41/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3168 - acc: 0.9034 - val_loss: 1.0585 - val_acc: 0.6748\n",
      "Epoch 42/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3176 - acc: 0.9034 - val_loss: 0.9198 - val_acc: 0.7323\n",
      "Epoch 43/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 306s 612ms/step - loss: 0.2827 - acc: 0.9146 - val_loss: 1.0626 - val_acc: 0.7107\n",
      "Epoch 44/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 0.2977 - acc: 0.9100 - val_loss: 1.1969 - val_acc: 0.6676\n",
      "Epoch 45/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 0.3730 - acc: 0.8821 - val_loss: 1.0023 - val_acc: 0.7095\n",
      "Epoch 46/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 0.3749 - acc: 0.8821 - val_loss: 3.3889 - val_acc: 0.2742\n",
      "Epoch 47/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 0.5095 - acc: 0.8353 - val_loss: 0.9053 - val_acc: 0.7185\n",
      "Epoch 48/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 0.3681 - acc: 0.8844 - val_loss: 1.1971 - val_acc: 0.6901\n",
      "Epoch 49/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3276 - acc: 0.8978 - val_loss: 1.1920 - val_acc: 0.6421\n",
      "Epoch 50/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3894 - acc: 0.8759 - val_loss: 1.0159 - val_acc: 0.6953\n",
      "Epoch 51/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3247 - acc: 0.8989 - val_loss: 1.0045 - val_acc: 0.7133\n",
      "Epoch 52/200\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3774 - acc: 0.8796 - val_loss: 1.1828 - val_acc: 0.6745\n",
      "Epoch 53/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3588 - acc: 0.8872 - val_loss: 1.0283 - val_acc: 0.6968\n",
      "Epoch 54/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3070 - acc: 0.9049 - val_loss: 1.2005 - val_acc: 0.6697\n",
      "Epoch 55/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3187 - acc: 0.9013 - val_loss: 0.8995 - val_acc: 0.7303\n",
      "Epoch 56/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.3279 - acc: 0.8971 - val_loss: 0.8910 - val_acc: 0.7340\n",
      "Epoch 57/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.2886 - acc: 0.9116 - val_loss: 0.8655 - val_acc: 0.7458\n",
      "Epoch 58/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.2694 - acc: 0.9175 - val_loss: 1.7656 - val_acc: 0.6036\n",
      "Epoch 59/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.2554 - acc: 0.9236 - val_loss: 1.3243 - val_acc: 0.6477\n",
      "Epoch 60/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.2357 - acc: 0.9301 - val_loss: 1.1170 - val_acc: 0.7072\n",
      "Epoch 61/200\n",
      "Learning rate:  0.001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.2363 - acc: 0.9311 - val_loss: 1.0214 - val_acc: 0.7195\n",
      "Epoch 62/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1825 - acc: 0.9487 - val_loss: 0.8646 - val_acc: 0.7747\n",
      "Epoch 63/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1743 - acc: 0.9529 - val_loss: 1.0779 - val_acc: 0.7306\n",
      "Epoch 64/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1756 - acc: 0.9515 - val_loss: 1.0992 - val_acc: 0.7401\n",
      "Epoch 65/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1819 - acc: 0.9489 - val_loss: 0.8640 - val_acc: 0.7684\n",
      "Epoch 66/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1826 - acc: 0.9487 - val_loss: 1.0370 - val_acc: 0.7459\n",
      "Epoch 67/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1708 - acc: 0.9533 - val_loss: 0.9983 - val_acc: 0.7623\n",
      "Epoch 68/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1781 - acc: 0.9507 - val_loss: 1.2412 - val_acc: 0.7026\n",
      "Epoch 69/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1822 - acc: 0.9489 - val_loss: 1.1033 - val_acc: 0.7282\n",
      "Epoch 70/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1686 - acc: 0.9541 - val_loss: 0.9955 - val_acc: 0.7559\n",
      "Epoch 71/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1549 - acc: 0.9585 - val_loss: 1.3194 - val_acc: 0.6999\n",
      "Epoch 72/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1638 - acc: 0.9554 - val_loss: 1.0829 - val_acc: 0.7298\n",
      "Epoch 73/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1686 - acc: 0.9533 - val_loss: 0.8687 - val_acc: 0.7904\n",
      "Epoch 74/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1733 - acc: 0.9514 - val_loss: 1.2162 - val_acc: 0.7001\n",
      "Epoch 75/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1839 - acc: 0.9488 - val_loss: 1.0145 - val_acc: 0.7551\n",
      "Epoch 76/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1502 - acc: 0.9597 - val_loss: 0.9353 - val_acc: 0.7756\n",
      "Epoch 77/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1334 - acc: 0.9657 - val_loss: 1.0275 - val_acc: 0.7558\n",
      "Epoch 78/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1319 - acc: 0.9671 - val_loss: 1.0896 - val_acc: 0.7539\n",
      "Epoch 79/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1298 - acc: 0.9670 - val_loss: 1.0053 - val_acc: 0.7661\n",
      "Epoch 80/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1319 - acc: 0.9662 - val_loss: 1.3412 - val_acc: 0.7046\n",
      "Epoch 81/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1394 - acc: 0.9640 - val_loss: 1.2823 - val_acc: 0.6964\n",
      "Epoch 82/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1435 - acc: 0.9623 - val_loss: 0.9558 - val_acc: 0.7705\n",
      "Epoch 83/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1222 - acc: 0.9698 - val_loss: 0.8979 - val_acc: 0.7811\n",
      "Epoch 84/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1259 - acc: 0.9684 - val_loss: 1.2215 - val_acc: 0.7472\n",
      "Epoch 85/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1273 - acc: 0.9677 - val_loss: 0.9943 - val_acc: 0.7730\n",
      "Epoch 86/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1232 - acc: 0.9689 - val_loss: 1.1561 - val_acc: 0.7516\n",
      "Epoch 87/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1191 - acc: 0.9696 - val_loss: 1.1357 - val_acc: 0.7522\n",
      "Epoch 88/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1352 - acc: 0.9645 - val_loss: 1.1344 - val_acc: 0.7398\n",
      "Epoch 89/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1337 - acc: 0.9659 - val_loss: 1.2093 - val_acc: 0.7310\n",
      "Epoch 90/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1444 - acc: 0.9626 - val_loss: 1.5806 - val_acc: 0.6630\n",
      "Epoch 91/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1605 - acc: 0.9558 - val_loss: 1.1103 - val_acc: 0.7334\n",
      "Epoch 92/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1444 - acc: 0.9627 - val_loss: 1.0468 - val_acc: 0.7603\n",
      "Epoch 93/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1165 - acc: 0.9725 - val_loss: 1.0680 - val_acc: 0.7633\n",
      "Epoch 94/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1214 - acc: 0.9696 - val_loss: 0.9889 - val_acc: 0.7753\n",
      "Epoch 95/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1127 - acc: 0.9721 - val_loss: 1.1039 - val_acc: 0.7628\n",
      "Epoch 96/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1056 - acc: 0.9751 - val_loss: 1.0074 - val_acc: 0.7721\n",
      "Epoch 97/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1124 - acc: 0.9727 - val_loss: 1.1541 - val_acc: 0.7392\n",
      "Epoch 98/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1181 - acc: 0.9700 - val_loss: 1.3067 - val_acc: 0.7246\n",
      "Epoch 99/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1330 - acc: 0.9658 - val_loss: 1.0033 - val_acc: 0.7689\n",
      "Epoch 100/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1305 - acc: 0.9668 - val_loss: 1.3580 - val_acc: 0.7118\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1267 - acc: 0.9685 - val_loss: 1.1567 - val_acc: 0.7456\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1531 - acc: 0.9588 - val_loss: 1.0466 - val_acc: 0.7620\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1226 - acc: 0.9696 - val_loss: 0.9792 - val_acc: 0.7808\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1272 - acc: 0.9674 - val_loss: 1.1195 - val_acc: 0.7409\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.2198 - acc: 0.9383 - val_loss: 1.1350 - val_acc: 0.7183\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.2575 - acc: 0.9233 - val_loss: 1.2496 - val_acc: 0.6784\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1904 - acc: 0.9463 - val_loss: 1.0819 - val_acc: 0.7526\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1833 - acc: 0.9480 - val_loss: 1.1298 - val_acc: 0.7364\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1775 - acc: 0.9507 - val_loss: 1.0537 - val_acc: 0.7519\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1535 - acc: 0.9587 - val_loss: 1.0470 - val_acc: 0.7468\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1618 - acc: 0.9556 - val_loss: 1.0738 - val_acc: 0.7611\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1417 - acc: 0.9632 - val_loss: 0.9097 - val_acc: 0.7824\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1458 - acc: 0.9622 - val_loss: 1.5672 - val_acc: 0.6614\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1839 - acc: 0.9497 - val_loss: 1.1566 - val_acc: 0.7306\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1641 - acc: 0.9542 - val_loss: 1.0249 - val_acc: 0.7533\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1645 - acc: 0.9544 - val_loss: 1.2284 - val_acc: 0.7288\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1536 - acc: 0.9582 - val_loss: 1.1855 - val_acc: 0.7291\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1456 - acc: 0.9625 - val_loss: 1.1917 - val_acc: 0.7351\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1726 - acc: 0.9517 - val_loss: 1.1404 - val_acc: 0.7210\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1780 - acc: 0.9495 - val_loss: 1.0670 - val_acc: 0.7506\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0008\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1318 - acc: 0.9649 - val_loss: 0.9850 - val_acc: 0.7723\n",
      "Epoch 122/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0924 - acc: 0.9801 - val_loss: 1.0166 - val_acc: 0.7777\n",
      "Epoch 123/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0999 - acc: 0.9773 - val_loss: 1.0380 - val_acc: 0.7708\n",
      "Epoch 124/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1062 - acc: 0.9745 - val_loss: 1.0837 - val_acc: 0.7680\n",
      "Epoch 125/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1071 - acc: 0.9745 - val_loss: 1.0220 - val_acc: 0.7765\n",
      "Epoch 126/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1039 - acc: 0.9759 - val_loss: 1.1271 - val_acc: 0.7644\n",
      "Epoch 127/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1064 - acc: 0.9745 - val_loss: 1.1967 - val_acc: 0.7446\n",
      "Epoch 128/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1275 - acc: 0.9678 - val_loss: 1.0762 - val_acc: 0.7727\n",
      "Epoch 129/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0992 - acc: 0.9775 - val_loss: 1.0105 - val_acc: 0.7857\n",
      "Epoch 130/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1001 - acc: 0.9766 - val_loss: 1.3322 - val_acc: 0.7367\n",
      "Epoch 131/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1027 - acc: 0.9762 - val_loss: 0.9682 - val_acc: 0.7853\n",
      "Epoch 132/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1104 - acc: 0.9747 - val_loss: 1.1700 - val_acc: 0.7519\n",
      "Epoch 133/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1042 - acc: 0.9755 - val_loss: 1.0074 - val_acc: 0.7863\n",
      "Epoch 134/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0858 - acc: 0.9816 - val_loss: 1.1378 - val_acc: 0.7722\n",
      "Epoch 135/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1001 - acc: 0.9771 - val_loss: 1.3941 - val_acc: 0.7139\n",
      "Epoch 136/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1174 - acc: 0.9714 - val_loss: 1.0578 - val_acc: 0.7681\n",
      "Epoch 137/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1014 - acc: 0.9769 - val_loss: 1.0417 - val_acc: 0.7787\n",
      "Epoch 138/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0983 - acc: 0.9775 - val_loss: 1.0031 - val_acc: 0.7777\n",
      "Epoch 139/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0849 - acc: 0.9825 - val_loss: 1.1488 - val_acc: 0.7711\n",
      "Epoch 140/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0921 - acc: 0.9797 - val_loss: 0.9865 - val_acc: 0.7881\n",
      "Epoch 141/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1360 - acc: 0.9654 - val_loss: 1.1712 - val_acc: 0.7550\n",
      "Epoch 142/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1105 - acc: 0.9735 - val_loss: 1.1350 - val_acc: 0.7618\n",
      "Epoch 143/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0912 - acc: 0.9802 - val_loss: 1.1601 - val_acc: 0.7628\n",
      "Epoch 144/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1090 - acc: 0.9740 - val_loss: 1.1793 - val_acc: 0.7581\n",
      "Epoch 145/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0892 - acc: 0.9812 - val_loss: 1.0565 - val_acc: 0.7828\n",
      "Epoch 146/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0824 - acc: 0.9835 - val_loss: 1.0030 - val_acc: 0.7893\n",
      "Epoch 147/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0891 - acc: 0.9809 - val_loss: 1.2747 - val_acc: 0.7484\n",
      "Epoch 148/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0984 - acc: 0.9775 - val_loss: 1.0802 - val_acc: 0.7770\n",
      "Epoch 149/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0847 - acc: 0.9822 - val_loss: 1.0455 - val_acc: 0.7881\n",
      "Epoch 150/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0743 - acc: 0.9861 - val_loss: 1.0191 - val_acc: 0.7859\n",
      "Epoch 151/200\n",
      "Learning rate:  0.0006400000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0853 - acc: 0.9821 - val_loss: 1.2880 - val_acc: 0.7481\n",
      "Epoch 152/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1046 - acc: 0.9758 - val_loss: 1.0685 - val_acc: 0.7731\n",
      "Epoch 153/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.1062 - acc: 0.9740 - val_loss: 1.2071 - val_acc: 0.7536\n",
      "Epoch 154/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0990 - acc: 0.9770 - val_loss: 1.1158 - val_acc: 0.7724\n",
      "Epoch 155/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0907 - acc: 0.9805 - val_loss: 1.3021 - val_acc: 0.7408\n",
      "Epoch 156/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0973 - acc: 0.9777 - val_loss: 1.3840 - val_acc: 0.7269\n",
      "Epoch 157/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0989 - acc: 0.9770 - val_loss: 1.2045 - val_acc: 0.7441\n",
      "Epoch 158/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0927 - acc: 0.9793 - val_loss: 1.1560 - val_acc: 0.7692\n",
      "Epoch 159/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0784 - acc: 0.9841 - val_loss: 1.0666 - val_acc: 0.7898\n",
      "Epoch 160/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0731 - acc: 0.9863 - val_loss: 1.1418 - val_acc: 0.7762\n",
      "Epoch 161/200\n",
      "Learning rate:  0.0006400000000000002\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0711 - acc: 0.9871 - val_loss: 1.1679 - val_acc: 0.7746\n",
      "Epoch 162/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0595 - acc: 0.9898 - val_loss: 0.9769 - val_acc: 0.7980\n",
      "Epoch 163/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0554 - acc: 0.9914 - val_loss: 1.0181 - val_acc: 0.7978\n",
      "Epoch 164/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0684 - acc: 0.9877 - val_loss: 1.1103 - val_acc: 0.7793\n",
      "Epoch 165/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0641 - acc: 0.9888 - val_loss: 0.9656 - val_acc: 0.8080\n",
      "Epoch 166/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0583 - acc: 0.9905 - val_loss: 1.1075 - val_acc: 0.7927\n",
      "Epoch 167/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0599 - acc: 0.9904 - val_loss: 1.3187 - val_acc: 0.7621\n",
      "Epoch 168/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0665 - acc: 0.9875 - val_loss: 1.1135 - val_acc: 0.7889\n",
      "Epoch 169/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0621 - acc: 0.9891 - val_loss: 1.0778 - val_acc: 0.7945\n",
      "Epoch 170/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0605 - acc: 0.9898 - val_loss: 1.2417 - val_acc: 0.7745\n",
      "Epoch 171/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0644 - acc: 0.9879 - val_loss: 1.2641 - val_acc: 0.7684\n",
      "Epoch 172/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0638 - acc: 0.9889 - val_loss: 1.1573 - val_acc: 0.7807\n",
      "Epoch 173/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0644 - acc: 0.9880 - val_loss: 1.0022 - val_acc: 0.8001\n",
      "Epoch 174/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0635 - acc: 0.9885 - val_loss: 0.9796 - val_acc: 0.8048\n",
      "Epoch 175/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0623 - acc: 0.9893 - val_loss: 1.1289 - val_acc: 0.7871\n",
      "Epoch 176/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0539 - acc: 0.9918 - val_loss: 1.1194 - val_acc: 0.7941\n",
      "Epoch 177/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0601 - acc: 0.9900 - val_loss: 1.3192 - val_acc: 0.7702\n",
      "Epoch 178/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0568 - acc: 0.9905 - val_loss: 1.1482 - val_acc: 0.7895\n",
      "Epoch 179/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0645 - acc: 0.9882 - val_loss: 1.1412 - val_acc: 0.7745\n",
      "Epoch 180/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0614 - acc: 0.9892 - val_loss: 1.1525 - val_acc: 0.7848\n",
      "Epoch 181/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0573 - acc: 0.9901 - val_loss: 0.9942 - val_acc: 0.8055\n",
      "Epoch 182/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0542 - acc: 0.9918 - val_loss: 1.1992 - val_acc: 0.7750\n",
      "Epoch 183/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0583 - acc: 0.9898 - val_loss: 1.1079 - val_acc: 0.7919\n",
      "Epoch 184/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0569 - acc: 0.9908 - val_loss: 1.3345 - val_acc: 0.7639\n",
      "Epoch 185/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0578 - acc: 0.9900 - val_loss: 0.9843 - val_acc: 0.8126\n",
      "Epoch 186/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0546 - acc: 0.9913 - val_loss: 1.1414 - val_acc: 0.7783\n",
      "Epoch 187/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0676 - acc: 0.9868 - val_loss: 1.4845 - val_acc: 0.7343\n",
      "Epoch 188/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0728 - acc: 0.9854 - val_loss: 1.0561 - val_acc: 0.7891\n",
      "Epoch 189/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0571 - acc: 0.9901 - val_loss: 1.2466 - val_acc: 0.7682\n",
      "Epoch 190/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0555 - acc: 0.9910 - val_loss: 1.0153 - val_acc: 0.7990\n",
      "Epoch 191/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0551 - acc: 0.9909 - val_loss: 1.1291 - val_acc: 0.7894\n",
      "Epoch 192/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0599 - acc: 0.9893 - val_loss: 1.4458 - val_acc: 0.7378\n",
      "Epoch 193/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0601 - acc: 0.9898 - val_loss: 1.2202 - val_acc: 0.7759\n",
      "Epoch 194/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0613 - acc: 0.9890 - val_loss: 1.0448 - val_acc: 0.8003\n",
      "Epoch 195/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0578 - acc: 0.9900 - val_loss: 1.2096 - val_acc: 0.7796\n",
      "Epoch 196/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0592 - acc: 0.9898 - val_loss: 1.0408 - val_acc: 0.8007\n",
      "Epoch 197/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0588 - acc: 0.9895 - val_loss: 1.1328 - val_acc: 0.7883\n",
      "Epoch 198/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0631 - acc: 0.9888 - val_loss: 1.2724 - val_acc: 0.7730\n",
      "Epoch 199/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0712 - acc: 0.9857 - val_loss: 1.4706 - val_acc: 0.7437\n",
      "Epoch 200/200\n",
      "Learning rate:  0.0005120000000000001\n",
      "500/500 [==============================] - 291s 582ms/step - loss: 0.0657 - acc: 0.9875 - val_loss: 1.1574 - val_acc: 0.7790\n"
     ]
    }
   ],
   "source": [
    "print(tent_wnn.summary())\n",
    "\n",
    "tent_wnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "data_augmentation = True\n",
    "\n",
    "if not data_augmentation:\n",
    "    tent_wnn.fit(x_train, to_categorical(y_train), batch_size=100, epochs=200, validation_data=(x_test, to_categorical(y_test)), shuffle=True,callbacks=callbacks)\n",
    "    \n",
    "\n",
    "else:\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    tent_wnn.fit_generator(datagen.flow(x_train, to_categorical(y_train),\n",
    "                                     batch_size=100),\n",
    "                        epochs=200, steps_per_epoch=500,\n",
    "                        validation_data=(x_test, to_categorical(y_test)),\n",
    "                        workers=4, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.45240587]]]\n",
      "[[[0.42183113]]]\n",
      "[[[0.05019424]]]\n",
      "[[[0.31066635]]]\n",
      "[[[0.06147592]]]\n",
      "[[[0.4985551]]]\n",
      "[[[0.06201436]]]\n",
      "[[[0.4171327]]]\n",
      "[[[0.06730621]]]\n",
      "[[[0.10366999]]]\n",
      "[[[0.05]]]\n",
      "[[[0.12733164]]]\n",
      "[[[0.05649449]]]\n",
      "[[[0.7183297]]]\n",
      "[[[0.7067606]]]\n",
      "[[[0.0659201]]]\n",
      "[[[0.05]]]\n",
      "[[[0.62916434]]]\n",
      "[[[0.9998009]]]\n",
      "[[[0.62269974]]]\n",
      "[[[0.9999301]]]\n",
      "[[[0.7188164]]]\n",
      "[[[0.90166104]]]\n",
      "[[[0.42692164]]]\n",
      "[[[0.92941064]]]\n"
     ]
    }
   ],
   "source": [
    "for lay in tent_wnn.layers:\n",
    "    if \"theta_initializer\" in lay.get_config():\n",
    "        #print(np.shape(lay.get_weights()))\n",
    "        print(lay.get_weights()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent_wnn mean confidence on train data  0.9732332\n",
      "tent_wnn mean confidence on test data  0.9314898\n",
      "\n",
      "\n",
      "\n",
      "tent_wnn mean confidence on random matrix  0.8055158\n",
      "tent_wnn mean confidence on flat_gray img  0.59138983\n",
      "tent_wnn mean confidence on avg_img  0.61460364\n"
     ]
    }
   ],
   "source": [
    "print(\"tent_wnn mean confidence on train data \",np.mean(np.max(tent_wnn.predict(x_train),axis=1)))\n",
    "print(\"tent_wnn mean confidence on test data \",np.mean(np.max(tent_wnn.predict(x_test),axis=1)))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "ran = np.random.random(size=(100,32,32,3))*255\n",
    "\n",
    "print(\"tent_wnn mean confidence on random matrix \",np.mean(np.max(tent_wnn.predict(ran),axis=1)))\n",
    "\n",
    "flat_gray=np.ones((100,32,32,3))*128\n",
    "print(\"tent_wnn mean confidence on flat_gray img \",np.mean(np.max(tent_wnn.predict(flat_gray),axis=1)))\n",
    "\n",
    "avg_img = np.mean(x_test, axis=0,keepdims=True)\n",
    "#print(np.shape(avg_img))\n",
    "print(\"tent_wnn mean confidence on avg_img \",np.mean(np.max(tent_wnn.predict(avg_img),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent mean confidence on cifar100  0.9314898\n"
     ]
    }
   ],
   "source": [
    "cifar100preds = tent_wnn.predict(x100_test)\n",
    "print(\"tent mean confidence on cifar100 \",np.mean(np.max(cifar100preds,axis=1)))\n",
    "#print(cifar100preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnn_28_10 = tent_wnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
