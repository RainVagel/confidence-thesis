{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.layers import Layer, InputSpec, ReLU\n",
    "from tensorflow.keras.initializers import Ones\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAct(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MAct, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.c = self.add_weight(name=\"c\",\n",
    "                                shape=(input_shape[1],),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True) # Initialiseerida c Ã¼htedeks / nullideks\n",
    "        self.b = self.add_weight(name=\"b\",\n",
    "                                shape=(input_shape[1],),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True) # Initialiseerida b nullideks\n",
    "        super(MAct, self).build(input_shape)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        first_exp = tf.exp(self.c - tf.abs(inputs))\n",
    "\n",
    "        p = (first_exp + tf.exp(self.b)) / tf.reduce_sum(first_exp + tf.exp(self.b), axis=1, keepdims=True)\n",
    "        \n",
    "        #p = tf.exp(inputs) / tf.reduce_sum(tf.exp(inputs), axis=0, keepdims=True)\n",
    "        return p\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(probs, y):\n",
    "    #losses = tf.nn.softmax_cross_entropy_with_logits(logits=probs, labels=y) # Tavaline CE\n",
    "    cce = CategoricalCrossentropy()\n",
    "    losses = cce(probs, y)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "\n",
    "def max_conf(probs, dim):\n",
    "    y = tf.argmax(probs, 1)\n",
    "    y = tf.one_hot(y, dim)\n",
    "    #losses = -tf.nn.softmax_cross_entropy_with_logits(logits=probs, labels=y) # Tavaline CE\n",
    "    cce = CategoricalCrossentropy()\n",
    "    losses = -cce(probs, y)\n",
    "    return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(x, dim):\n",
    "    eps = 0.025\n",
    "    n_iters = 4\n",
    "    step_size = 0.02\n",
    "\n",
    "    unif = tf.random.uniform(minval=-eps, maxval=eps, shape=tf.shape(x))\n",
    "    x_adv = tf.clip_by_value(x + unif, 0., 1.)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        x_adv = tf.Variable(x_adv)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = max_conf(model(x_adv), dim)\n",
    "            grad = tape.gradient(loss, x_adv)\n",
    "            g = tf.sign(grad)\n",
    "\n",
    "        x_adv_start = x_adv + step_size*g\n",
    "        x_adv = tf.clip_by_value(x_adv, 0., 1.)\n",
    "        delta = x_adv - x_adv_start\n",
    "        delta = tf.clip_by_value(delta, -eps, eps)\n",
    "        x_adv = x_adv_start + delta\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, plot_min, plot_max, max_prob, name, n_iters, MAct):\n",
    "    n_grid = 200\n",
    "    x_plot = np.linspace(plot_min, plot_max, n_grid)\n",
    "    y_plot = np.linspace(plot_min, plot_max, n_grid)\n",
    "    \n",
    "    points = []\n",
    "    for xx in x_plot:\n",
    "        for yy in y_plot:\n",
    "            points.append((yy, xx))\n",
    "    points = np.array(points)\n",
    "    \n",
    "    if MAct:\n",
    "        probs = model(points).numpy()\n",
    "    else:\n",
    "        logits = model(points) # For not MAct\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "    if max_prob:\n",
    "        z_plot = probs.max(1)\n",
    "    else:\n",
    "        z_plot = probs[:, 0]\n",
    "    z_plot = z_plot.reshape(len(x_plot), len(y_plot)) * 100\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    vmax = 100\n",
    "    vmin = 50 if max_prob else 0\n",
    "    plt.contourf(x_plot, y_plot, z_plot, levels=np.linspace(50, 100, 50))\n",
    "    cbar = plt.colorbar(ticks=np.linspace(vmin, vmax, 6))\n",
    "    \n",
    "    cbar.ax.set_title('confidence', fontsize=12, pad=12)\n",
    "    cbar.set_ticklabels(['50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "    \n",
    "    y_np = np.array(y)\n",
    "    X0 = X[y_np.argmax(1)==0]\n",
    "    X1 = X[y_np.argmax(1)==1]\n",
    "    plt.scatter(X0[:, 0], X0[:, 1], s=20, edgecolors='red', facecolor='None',\n",
    "                marker='o', linewidths=0.2)\n",
    "    plt.scatter(X1[:, 0], X1[:, 1], s=20, edgecolors='green', facecolor='None',\n",
    "                marker='s', linewidths=0.2)\n",
    "    plt.xlim([plot_min, plot_max])\n",
    "    plt.ylim([plot_min, plot_max])\n",
    "    \n",
    "    margin = 0.01\n",
    "    #rect = matplotlib.patches.Rectangle((-margin, -margin), 1.0+2*margin, 1.0+2*margin, \n",
    "    #                                    linewidth=1.5, color='white', fill=False)\n",
    "    #ax.add_patch(rect)\n",
    "    \n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.savefig('two_moons_four_paper/{}_{:.1f}_{:.1f}_iters={}_max_prob={}.pdf'.format(\n",
    "        name, plot_min, plot_max, n_iters, max_prob), transparent=True)\n",
    "    plt.clf()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(200,input_shape=(2,)),\n",
    "    Activation('relu'),\n",
    "    Dense(100),\n",
    "    Activation('relu'),\n",
    "    Dense(50),\n",
    "    Activation('relu'),\n",
    "    Dense(25),\n",
    "    Activation('relu'),\n",
    "    #Dense(10),\n",
    "    #Activation('tanh'),\n",
    "    #Dense(100),\n",
    "    #Activation('selu'),\n",
    "    Dense(2),\n",
    "    #MAct(),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "# More noise in the moons makes the task harder\n",
    "X, y = datasets.make_moons(n_samples=2000, shuffle=True, noise=.02)\n",
    "# Rescale and shift the dataset to better fit into zero-one box\n",
    "X = (X + 1.6) / 4\n",
    "X[:, 0] = X[:, 0] - 0.035\n",
    "X[:, 1] = (X[:, 1] - 0.17) * 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extras_generator(X, y, x_lat_mult, x_long_mult):\n",
    "    X_extra, y_extra = datasets.make_moons(n_samples=2000, shuffle=True, noise=.02)\n",
    "    X_extra = (X_extra + 1.6) / 4\n",
    "    X_extra[:, 0] = X_extra[:, 0] - 0.035\n",
    "    X_extra[:, 1] = (X_extra[:, 1] - 0.17) * 1.75\n",
    "    X_extra[:, 0] = X_extra[:,0] + x_lat_mult\n",
    "    X_extra[:, 1] = X_extra[:, 1] + x_long_mult\n",
    "    X = np.append(X, X_extra, axis=0)\n",
    "    y = np.append(y, y_extra, axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extras_generator(X, y, 1, 0)\n",
    "X, y = extras_generator(X, y, 0, 1)\n",
    "X, y = extras_generator(X, y, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, y_test = datasets.make_moons(n_samples=400, shuffle=True, noise=.02)\n",
    "#X_test = (X_test + 1.6) / 4\n",
    "#X_test[:, 0] = X_test[:, 0] - 0.035\n",
    "#X_test[:, 1] = (X_test[:, 1] - 0.17) * 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 1]\n",
      "tf.Tensor(\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]], shape=(8000, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "y = tf.one_hot(y, dim)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "acet = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if acet:\n",
    "    n_iter = 900\n",
    "else:\n",
    "    n_iter = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Iter 100: loss_main=5.1294589043 loss_acet=-0.000 err=31.87%\n",
      "Iter 200: loss_main=4.7337722778 loss_acet=-0.000 err=29.36%\n",
      "Iter 300: loss_main=4.6478338242 loss_acet=-0.000 err=28.84%\n",
      "Iter 400: loss_main=3.9584867954 loss_acet=-0.000 err=24.51%\n",
      "Iter 500: loss_main=2.8115282059 loss_acet=-0.000 err=17.36%\n",
      "Iter 600: loss_main=2.4040732384 loss_acet=-0.000 err=14.85%\n",
      "Iter 700: loss_main=2.8179874420 loss_acet=-0.000 err=17.45%\n",
      "Iter 800: loss_main=2.5036611557 loss_acet=-0.000 err=15.50%\n",
      "Iter 900: loss_main=2.4397156239 loss_acet=-0.000 err=15.12%\n",
      "Iter 1000: loss_main=2.0667972565 loss_acet=-0.000 err=12.81%\n",
      "Iter 1100: loss_main=2.0148167610 loss_acet=-0.000 err=12.50%\n",
      "Iter 1200: loss_main=2.0148053169 loss_acet=-0.000 err=12.50%\n",
      "Iter 1300: loss_main=2.0147991180 loss_acet=-0.000 err=12.50%\n",
      "Iter 1400: loss_main=2.0147945881 loss_acet=-0.000 err=12.50%\n",
      "Iter 1500: loss_main=2.0147912502 loss_acet=-0.000 err=12.50%\n",
      "Iter 1600: loss_main=2.0147888660 loss_acet=-0.000 err=12.50%\n",
      "Iter 1700: loss_main=2.0147864819 loss_acet=-0.000 err=12.50%\n",
      "Iter 1800: loss_main=2.0147848129 loss_acet=-0.000 err=12.50%\n",
      "Iter 1900: loss_main=2.0147831440 loss_acet=-0.000 err=12.50%\n",
      "Iter 2000: loss_main=2.0147817135 loss_acet=-0.000 err=12.50%\n",
      "Iter 2100: loss_main=2.0147805214 loss_acet=-0.000 err=12.50%\n",
      "Iter 2200: loss_main=2.0147793293 loss_acet=-0.000 err=12.50%\n",
      "Iter 2300: loss_main=2.0147783756 loss_acet=-0.000 err=12.50%\n",
      "Iter 2400: loss_main=2.0147774220 loss_acet=-0.000 err=12.50%\n",
      "Iter 2500: loss_main=2.0147767067 loss_acet=-0.000 err=12.50%\n",
      "Iter 2600: loss_main=2.0147759914 loss_acet=-0.000 err=12.50%\n",
      "Iter 2700: loss_main=2.0147750378 loss_acet=-0.000 err=12.50%\n",
      "Iter 2800: loss_main=2.0147745609 loss_acet=-0.000 err=12.50%\n",
      "Iter 2900: loss_main=2.0147740841 loss_acet=-0.000 err=12.50%\n",
      "Iter 3000: loss_main=2.0147733688 loss_acet=-0.000 err=12.50%\n",
      "Iter 3100: loss_main=2.0147728920 loss_acet=-0.000 err=12.50%\n",
      "Iter 3200: loss_main=2.0147724152 loss_acet=-0.000 err=12.50%\n",
      "Iter 3300: loss_main=2.0147719383 loss_acet=-0.000 err=12.50%\n",
      "Iter 3400: loss_main=2.0147714615 loss_acet=-0.000 err=12.50%\n",
      "Iter 3500: loss_main=2.0147712231 loss_acet=-0.000 err=12.50%\n",
      "Iter 3600: loss_main=2.0147707462 loss_acet=-0.000 err=12.50%\n",
      "Iter 3700: loss_main=2.0147702694 loss_acet=-0.000 err=12.50%\n",
      "Iter 3800: loss_main=2.0147700310 loss_acet=-0.000 err=12.50%\n",
      "Iter 3900: loss_main=2.0147695541 loss_acet=-0.000 err=12.50%\n",
      "Iter 4000: loss_main=2.0147693157 loss_acet=-0.000 err=12.50%\n"
     ]
    }
   ],
   "source": [
    "weights_list = []\n",
    "info_list = []\n",
    "\n",
    "# Custom training cycle going through the entire dataset\n",
    "for epoch in range(1, n_iter+1):\n",
    "    X_noise = tf.random.uniform([2*X.shape[0], X.shape[1]])\n",
    "    # If we use the ACET method, then adversarial noise will be generated\n",
    "    if acet:\n",
    "        X_noise = gen_adv(X_noise, dim)\n",
    "    # Context used to calculate the gradients of the model\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(X)\n",
    "        logits_noise = model(X_noise)\n",
    "        loss_main = cross_ent(logits, y)\n",
    "        loss_acet = acet * max_conf(logits_noise, dim)\n",
    "        loss = loss_main + loss_acet\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    if epoch % 100  == 0:\n",
    "        train_err = np.mean(logits.numpy().argmax(1) != y.numpy().argmax(1))\n",
    "        print(\"Iter {:03d}: loss_main={:.10f} loss_acet={:.3f} err={:.2%}\"\n",
    "              .format(epoch, loss_main, loss_acet, train_err))\n",
    "        \n",
    "        weights = model.layers[-1].get_weights()\n",
    "        info_list.append(\"Iter {:03d}: loss_main={:.10f} loss_acet={:.6f} err={:.2%}\"\n",
    "                         .format(epoch, loss_main, loss_acet, train_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"four_moons_paper_2_no_mact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"two_moons_four_paper/{}_iters={}.csv\".format(name, n_iter)\n",
    "with open(file_name, 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, delimiter=\"\\n\")\n",
    "    wr.writerow(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(model, 0.0, 2.0, max_prob=True,name=name, n_iters=n_iter, MAct=True)\n",
    "#plot(model, 0.3, 0.5, max_prob=True)\n",
    "#plot(model, -2.0, 3.0, max_prob=True)\n",
    "plot(model, -5.0, 6.0, max_prob=True,name=name, n_iters=n_iter, MAct=True)\n",
    "plot(model, -10.0, 10.0, max_prob=True,name=name, n_iters=n_iter, MAct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense 200, dense 100, dense 50 selu MAct lrate 0.01 Ãµppid tublisti, aga viimasel 100 iteratsioonil lÃ¤ks katki\n",
    "Dense 200, dense 100, dense 50 tanh MAct lrate 0.01 Ãµpib hÃ¤sti\n",
    "Dense 200, dense 100, dense 50 tanh MAct lrate 0.01 zeros ei Ãµpi nii hÃ¤sti, kui initialiseeritud Ã¼htedeks Ãµpib hÃ¤sti\n",
    "Dense 200, dense 100, dense 50, dense 25 tanh MAct lrate 0.01 zeros Ãµpib paremini\n",
    "Dense 200, dense 100, dense 50, dense 25 tanh MAct lrate 0.01 ei Ãµppinud Ã¼ldse\n",
    "Dense200_Dense100_Dense50_Dense25_Dense10_tanh_MAct_lrate0.01_zeros Ãµppis nÃµrgalt\n",
    "Dense200_Dense100_Dense50_Dense25_tanh_MAct_lrate0.01_zeros 3000 iteratsioon. Pikalt oli nÃµrk, aga lÃµpus leidis optimumi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ task_instance.xcom_pull(task_ids='Yolo', key='return_value')[0] }}\n"
     ]
    }
   ],
   "source": [
    "def pull(model):\n",
    "    return \"{{ task_instance.xcom_pull(task_ids='%s', key='return_value')[0] }}\" % model\n",
    "\n",
    "print(pull('Yolo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
