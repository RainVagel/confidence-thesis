{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.layers import Layer, InputSpec, ReLU\n",
    "from tensorflow.keras.initializers import Ones\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAct(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MAct, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.c = self.add_weight(name=\"c\",\n",
    "                                shape=(input_shape[1],),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True) # Initialiseerida c Ã¼htedeks / nullideks\n",
    "        self.b = self.add_weight(name=\"b\",\n",
    "                                shape=(input_shape[1],),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True) # Initialiseerida b nullideks\n",
    "        super(MAct, self).build(input_shape)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        first_exp = tf.exp(self.c - tf.square(inputs))\n",
    "\n",
    "        p = (first_exp + tf.exp(self.b)) / tf.reduce_sum(first_exp + tf.exp(self.b), axis=1, keepdims=True)\n",
    "        \n",
    "        #p = tf.exp(inputs) / tf.reduce_sum(tf.exp(inputs), axis=0, keepdims=True)\n",
    "        return p\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(probs, y):\n",
    "    #losses = tf.nn.softmax_cross_entropy_with_logits(logits=probs, labels=y) # Tavaline CE\n",
    "    cce = CategoricalCrossentropy()\n",
    "    losses = cce(probs, y)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "\n",
    "def max_conf(probs, dim):\n",
    "    y = tf.argmax(probs, 1)\n",
    "    y = tf.one_hot(y, dim)\n",
    "    #losses = -tf.nn.softmax_cross_entropy_with_logits(logits=probs, labels=y) # Tavaline CE\n",
    "    cce = CategoricalCrossentropy()\n",
    "    losses = -cce(probs, y)\n",
    "    return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(x, dim):\n",
    "    eps = 0.025\n",
    "    n_iters = 4\n",
    "    step_size = 0.02\n",
    "\n",
    "    unif = tf.random.uniform(minval=-eps, maxval=eps, shape=tf.shape(x))\n",
    "    x_adv = tf.clip_by_value(x + unif, 0., 1.)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        x_adv = tf.Variable(x_adv)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = max_conf(model(x_adv), dim)\n",
    "            grad = tape.gradient(loss, x_adv)\n",
    "            g = tf.sign(grad)\n",
    "\n",
    "        x_adv_start = x_adv + step_size*g\n",
    "        x_adv = tf.clip_by_value(x_adv, 0., 1.)\n",
    "        delta = x_adv - x_adv_start\n",
    "        delta = tf.clip_by_value(delta, -eps, eps)\n",
    "        x_adv = x_adv_start + delta\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, plot_min, plot_max, max_prob, name, n_iters, MAct):\n",
    "    n_grid = 200\n",
    "    x_plot = np.linspace(plot_min, plot_max, n_grid)\n",
    "    y_plot = np.linspace(plot_min, plot_max, n_grid)\n",
    "    \n",
    "    points = []\n",
    "    for xx in x_plot:\n",
    "        for yy in y_plot:\n",
    "            points.append((yy, xx))\n",
    "    points = np.array(points)\n",
    "    \n",
    "    if MAct:\n",
    "        probs = model(points).numpy()\n",
    "    else:\n",
    "        logits = model(points) # For not MAct\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "    if max_prob:\n",
    "        z_plot = probs.max(1)\n",
    "    else:\n",
    "        z_plot = probs[:, 0]\n",
    "    z_plot = z_plot.reshape(len(x_plot), len(y_plot)) * 100\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    vmax = 100\n",
    "    vmin = 50 if max_prob else 0\n",
    "    plt.contourf(x_plot, y_plot, z_plot, levels=np.linspace(50, 100, 50))\n",
    "    cbar = plt.colorbar(ticks=np.linspace(vmin, vmax, 6))\n",
    "    \n",
    "    cbar.ax.set_title('confidence', fontsize=12, pad=12)\n",
    "    cbar.set_ticklabels(['50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "    \n",
    "    y_np = np.array(y)\n",
    "    X0 = X[y_np.argmax(1)==0]\n",
    "    X1 = X[y_np.argmax(1)==1]\n",
    "    plt.scatter(X0[:, 0], X0[:, 1], s=20, edgecolors='red', facecolor='None',\n",
    "                marker='o', linewidths=0.2)\n",
    "    plt.scatter(X1[:, 0], X1[:, 1], s=20, edgecolors='green', facecolor='None',\n",
    "                marker='s', linewidths=0.2)\n",
    "    plt.xlim([plot_min, plot_max])\n",
    "    plt.ylim([plot_min, plot_max])\n",
    "    \n",
    "    margin = 0.01\n",
    "    #rect = matplotlib.patches.Rectangle((-margin, -margin), 1.0+2*margin, 1.0+2*margin, \n",
    "    #                                    linewidth=1.5, color='white', fill=False)\n",
    "    #ax.add_patch(rect)\n",
    "    \n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.savefig('two_moons_four/{}_{:.1f}_{:.1f}_iters={}_max_prob={}.pdf'.format(\n",
    "        name, plot_min, plot_max, n_iters, max_prob), transparent=True)\n",
    "    plt.clf()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MAct.call of <__main__.MAct object at 0x7fcfd3902438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MAct.call of <__main__.MAct object at 0x7fcfd3902438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(200,input_shape=(2,)),\n",
    "    Activation('tanh'),\n",
    "    Dense(100),\n",
    "    Activation('tanh'),\n",
    "    Dense(50),\n",
    "    Activation('tanh'),\n",
    "    Dense(25),\n",
    "    Activation('tanh'),\n",
    "    #Dense(10),\n",
    "    #Activation('tanh'),\n",
    "    #Dense(100),\n",
    "    #Activation('selu'),\n",
    "    Dense(2),\n",
    "    MAct(),\n",
    "    #Activation('softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "# More noise in the moons makes the task harder\n",
    "X, y = datasets.make_moons(n_samples=2000, shuffle=True, noise=.02)\n",
    "# Rescale and shift the dataset to better fit into zero-one box\n",
    "X = (X + 1.6) / 4\n",
    "X[:, 0] = X[:, 0] - 0.035\n",
    "X[:, 1] = (X[:, 1] - 0.17) * 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extras_generator(X, y, x_lat_mult, x_long_mult):\n",
    "    X_extra, y_extra = datasets.make_moons(n_samples=2000, shuffle=True, noise=.02)\n",
    "    X_extra = (X_extra + 1.6) / 4\n",
    "    X_extra[:, 0] = X_extra[:, 0] - 0.035\n",
    "    X_extra[:, 1] = (X_extra[:, 1] - 0.17) * 1.75\n",
    "    X_extra[:, 0] = X_extra[:,0] + x_lat_mult\n",
    "    X_extra[:, 1] = X_extra[:, 1] + x_long_mult\n",
    "    X = np.append(X, X_extra, axis=0)\n",
    "    y = np.append(y, y_extra, axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extras_generator(X, y, 1, 0)\n",
    "X, y = extras_generator(X, y, 0, 1)\n",
    "X, y = extras_generator(X, y, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, y_test = datasets.make_moons(n_samples=400, shuffle=True, noise=.02)\n",
    "#X_test = (X_test + 1.6) / 4\n",
    "#X_test[:, 0] = X_test[:, 0] - 0.035\n",
    "#X_test[:, 1] = (X_test[:, 1] - 0.17) * 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 1 0]\n",
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]], shape=(8000, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "y = tf.one_hot(y, dim)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "acet = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "if acet:\n",
    "    n_iter = 900\n",
    "else:\n",
    "    n_iter = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_129 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Iter 100: loss_main=5.0285615921 loss_acet=-0.000 err=21.88%\n",
      "Iter 200: loss_main=2.7243061066 loss_acet=-0.000 err=19.15%\n",
      "Iter 300: loss_main=1.1979845762 loss_acet=-0.000 err=3.25%\n",
      "Iter 400: loss_main=1.1873925924 loss_acet=-0.000 err=5.71%\n",
      "Iter 500: loss_main=0.1842817962 loss_acet=-0.000 err=0.00%\n",
      "Iter 600: loss_main=0.1316006780 loss_acet=-0.000 err=0.00%\n",
      "Iter 700: loss_main=0.0999560580 loss_acet=-0.000 err=0.00%\n",
      "Iter 800: loss_main=0.0790352970 loss_acet=-0.000 err=0.00%\n",
      "Iter 900: loss_main=0.0643220991 loss_acet=-0.000 err=0.00%\n",
      "Iter 1000: loss_main=0.0535021573 loss_acet=-0.000 err=0.00%\n",
      "Iter 1100: loss_main=0.0452697761 loss_acet=-0.000 err=0.00%\n",
      "Iter 1200: loss_main=0.0388352126 loss_acet=-0.000 err=0.00%\n",
      "Iter 1300: loss_main=0.0336950272 loss_acet=-0.000 err=0.00%\n",
      "Iter 1400: loss_main=0.0295137782 loss_acet=-0.000 err=0.00%\n",
      "Iter 1500: loss_main=0.0260602590 loss_acet=-0.000 err=0.00%\n",
      "Iter 1600: loss_main=0.0231704675 loss_acet=-0.000 err=0.00%\n",
      "Iter 1700: loss_main=0.0207250249 loss_acet=-0.000 err=0.00%\n",
      "Iter 1800: loss_main=0.0186351929 loss_acet=-0.000 err=0.00%\n",
      "Iter 1900: loss_main=0.0168338735 loss_acet=-0.000 err=0.00%\n",
      "Iter 2000: loss_main=0.0152691556 loss_acet=-0.000 err=0.00%\n"
     ]
    }
   ],
   "source": [
    "weights_list = []\n",
    "info_list = []\n",
    "\n",
    "# Custom training cycle going through the entire dataset\n",
    "for epoch in range(1, n_iter+1):\n",
    "    X_noise = tf.random.uniform([2*X.shape[0], X.shape[1]])\n",
    "    # If we use the ACET method, then adversarial noise will be generated\n",
    "    if acet:\n",
    "        X_noise = gen_adv(X_noise, dim)\n",
    "    # Context used to calculate the gradients of the model\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(X)\n",
    "        logits_noise = model(X_noise)\n",
    "        loss_main = cross_ent(logits, y)\n",
    "        loss_acet = acet * max_conf(logits_noise, dim)\n",
    "        loss = loss_main + loss_acet\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    if epoch % 100  == 0:\n",
    "        train_err = np.mean(logits.numpy().argmax(1) != y.numpy().argmax(1))\n",
    "        print(\"Iter {:03d}: loss_main={:.10f} loss_acet={:.3f} err={:.2%}\"\n",
    "              .format(epoch, loss_main, loss_acet, train_err))\n",
    "        \n",
    "        weights = model.layers[-1].get_weights()\n",
    "        info_list.append(\"Iter {:03d}: loss_main={:.10f} loss_acet={:.6f} err={:.2%} c: {}, b: {}\"\n",
    "                         .format(epoch, loss_main, loss_acet, train_err, weights[0], weights[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Dense200_Dense100_Dense50_Dense25_tanh_MAct_lrate0.01_zeros\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"two_moons_four/{}_iters={}.csv\".format(name, n_iter)\n",
    "with open(file_name, 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, delimiter=\"\\n\")\n",
    "    wr.writerow(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(model, 0.0, 2.0, max_prob=True,name=name, n_iters=n_iter, MAct=True)\n",
    "#plot(model, 0.3, 0.5, max_prob=True)\n",
    "#plot(model, -2.0, 3.0, max_prob=True)\n",
    "plot(model, -5.0, 6.0, max_prob=True,name=name, n_iters=n_iter, MAct=True)\n",
    "plot(model, -10.0, 10.0, max_prob=True,name=name, n_iters=n_iter, MAct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense 200, dense 100, dense 50 selu MAct lrate 0.01 Ãµppid tublisti, aga viimasel 100 iteratsioonil lÃ¤ks katki\n",
    "Dense 200, dense 100, dense 50 tanh MAct lrate 0.01 Ãµpib hÃ¤sti\n",
    "Dense 200, dense 100, dense 50 tanh MAct lrate 0.01 zeros ei Ãµpi nii hÃ¤sti, kui initialiseeritud Ã¼htedeks Ãµpib hÃ¤sti\n",
    "Dense 200, dense 100, dense 50, dense 25 tanh MAct lrate 0.01 zeros Ãµpib paremini\n",
    "Dense 200, dense 100, dense 50, dense 25 tanh MAct lrate 0.01 ei Ãµppinud Ã¼ldse\n",
    "Dense200_Dense100_Dense50_Dense25_Dense10_tanh_MAct_lrate0.01_zeros Ãµppis nÃµrgalt\n",
    "Dense200_Dense100_Dense50_Dense25_tanh_MAct_lrate0.01_zeros 3000 iteratsioon. Pikalt oli nÃµrk, aga lÃµpus leidis optimumi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
